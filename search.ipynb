{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/govind17/Information-Retrieval-Project/blob/main/LDA_feature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U Flask\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install matplotlib\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/GovindShukla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/GovindShukla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One time installation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import io\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import flask\n",
    "from flask import request\n",
    "from flask_cors import CORS\n",
    "from flask import Flask, Response\n",
    "from keyPhrasification import key_phrasification\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "# import pyterrier as pt\n",
    "# from pyterrier_t5 import MonoT5ReRanker\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import ast\n",
    "from keyPhrasification import key_phrasification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Indexing the cord-19 dataset\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = 'C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/terrier_cord19'\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # create the index, using the IterDictIndexer indexer\n",
    "    indexer = pt.index.IterDictIndexer(pt_index_path)\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    index_ref = indexer.index(cord19.get_corpus_iter(),\n",
    "                              fields=('abstract',),\n",
    "                              meta=('docno',))\n",
    "else:\n",
    "    # if you already have the index, use it.\n",
    "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "    index = pt.IndexFactory.of(index_ref)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rocchio algorithm\n",
    "def rocchio_algorithm(query_doc_vector, docs_relevant_vectors,\n",
    "                      docs_irrelevant_vectors, key_relevant_vectors,\n",
    "                      key_irrelevant_vectors,\n",
    "                      alpha, beta, gamma, delta):\n",
    "\n",
    "    # sum_of_rel_doc_vectors = docs_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_doc_vectors', sum_of_rel_doc_vectors)\n",
    "    sum_of_rel_doc_vectors=[]\n",
    "    sum_of_irrel_doc_vectors=[]\n",
    "    sum_of_rel_key_vectors=[]\n",
    "    sum_of_irrel_key_vectors=[]\n",
    "    for each_rel_doc_vector in docs_relevant_vectors:\n",
    "        if len(sum_of_rel_doc_vectors) == 0:\n",
    "            print('Length of sum_of_rel_doc_vectors is zero')\n",
    "            sum_of_rel_doc_vectors = each_rel_doc_vector\n",
    "        else:\n",
    "            # print(each_rel_doc_vector)\n",
    "            sum_of_rel_doc_vectors = list(map(operator.add, sum_of_rel_doc_vectors, each_rel_doc_vector))\n",
    "            # print('SUM :', sum_of_rel_doc_vectors)\n",
    "    for each_irrel_doc_vector in docs_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_doc_vectors) == 0:\n",
    "            print('Length of sum_of_irrel_doc_vectors is zero')\n",
    "            sum_of_irrel_doc_vectors = each_irrel_doc_vector\n",
    "        else:\n",
    "            sum_of_irrel_doc_vectors = list(map(operator.add, sum_of_irrel_doc_vectors, each_irrel_doc_vector))\n",
    "    # sum_of_irrel_doc_vectors = docs_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_doc_vectors', sum_of_irrel_doc_vectors)\n",
    "    for each_rel_key_vector in key_relevant_vectors:\n",
    "        if len(sum_of_rel_key_vectors) == 0:\n",
    "            print('Length of sum_of_rel_key_vectors is zero')\n",
    "            sum_of_rel_key_vectors = each_rel_key_vector\n",
    "        else:\n",
    "            sum_of_rel_key_vectors = list(map(operator.add, sum_of_rel_key_vectors, each_rel_key_vector))\n",
    "    # sum_of_rel_key_vectors = key_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_key_vectors', sum_of_rel_key_vectors)\n",
    "    for each_irrel_key_vector in key_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_key_vectors) == 0:\n",
    "            print('Length of sum_of_irrel_key_vectors is zero')\n",
    "            sum_of_irrel_key_vectors = each_irrel_key_vector\n",
    "        else:\n",
    "            sum_of_irrel_key_vectors = list(map(operator.add, sum_of_irrel_key_vectors, each_irrel_key_vector))\n",
    "    # sum_of_irrel_key_vectors = key_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_key_vectors', (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors))\n",
    "\n",
    "    new_doc_vector_query = np.sum([np.array(query_doc_vector)\n",
    "    , (alpha/len(docs_relevant_vectors)) * np.array(sum_of_rel_doc_vectors)\n",
    "    , (beta/len(docs_irrelevant_vectors)) * np.array(sum_of_irrel_doc_vectors)\n",
    "    , (gamma/len(docs_relevant_vectors)) * np.array(sum_of_rel_key_vectors)\n",
    "    , (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors)], axis=0)\n",
    "    df = pd.DataFrame({\"a\": [new_doc_vector_query]})\n",
    "    return df.values\n",
    "\n",
    "# Compute cosine scores\n",
    "def compute_cosine_sim(new_query_vector, all_data, name):\n",
    "    consine_similarities = []\n",
    "    for index, data in all_data.iterrows():\n",
    "        cosine_sim = cosine_similarity([np.array(new_query_vector[0])], [np.array(all_data[name][index])])\n",
    "        consine_similarities.append(cosine_sim[0][0])\n",
    "    return consine_similarities\n",
    "\n",
    "# Rank data\n",
    "def rank_data(new_query_vector, dataset, name):\n",
    "  cosine_sim_values = compute_cosine_sim(new_query_vector, dataset, name)\n",
    "  dataset['Cosine_Similarity_' + name] = cosine_sim_values\n",
    "  sorted_dataset = dataset.sort_values(by=['Cosine_Similarity_' + name], ascending=False)\n",
    "\n",
    "  return sorted_dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "def pre_processing_data(data):\n",
    "    ps = PorterStemmer()\n",
    "    tagged_dataset = [TaggedDocument(words=[ps.stem(w) for w in nltk.word_tokenize(str(_d)) if word_tokenize(str(_d).lower()) not in stopwords.words('english')], tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    #tagged_dataset = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    return tagged_dataset\n",
    "\n",
    "def model_doc2vec(model, tagged_data, num_epochs):\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,total_examples=len(tagged_data), epochs=num_epochs)\n",
    "    return model\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "def define_d2v_model(dataset):\n",
    "    preprocessed_tagged_dataset = pre_processing_data(dataset)\n",
    "    model_d2v = Doc2Vec(dm=1, vector_size=100, window = 10, negative=5, hs=0, min_count=2, sample = 0, alpha=0.025, min_alpha=0.001, dm_mean = 0, dbow_words=1)\n",
    "    model = model_doc2vec(model_d2v, preprocessed_tagged_dataset, 100)\n",
    "    return model\n",
    "\n",
    "def get_vectors(input_data, model):\n",
    "    preprocessed_tagged_input_data= pre_processing_data(input_data)\n",
    "    id, vectors = np.array(vector_for_learning(model, preprocessed_tagged_input_data), dtype=object)\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "monoT5 = MonoT5ReRanker(text_field='abstract')\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "cord19_df = pd.read_csv('C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/cord19_sum_key.csv')\n",
    "# model = define_d2v_model(cord19_df['abstract'])\n",
    "model = joblib.load('gensim_model_2.pkl')\n",
    "query_df = pd.DataFrame\n",
    "reranked_df = pd.DataFrame\n",
    "\n",
    "def search_query(query):\n",
    "  index_ref2 = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "  index2 = pt.IndexFactory.of(index_ref2)\n",
    "  # print(query)\n",
    "  if not pt.started():\n",
    "      pt.init()\n",
    "  br = pt.BatchRetrieve(index2) % 15\n",
    "  pipeline = (br >> pt.text.get_text(dataset, 'abstract') >> monoT5)\n",
    "  search_result = pipeline.search(query)\n",
    "  search_result = search_result.drop_duplicates(subset=['docno'])\n",
    "  print(search_result)\n",
    "  filtered_docs = pd.merge(cord19_df, search_result, on = \"docno\", how = \"inner\")\n",
    "\n",
    "  return filtered_docs.head(10)\n",
    "\n",
    "\n",
    "# search_query('covid')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_reranking(feedback_df : pd.DataFrame):\n",
    "    global  query_df\n",
    "    global reranked_df\n",
    "    feedback_df[\"summary_vec\"] = get_vectors(feedback_df['summary'], model)\n",
    "    feedback_df[\"keyList_vec\"] = get_vectors(feedback_df['KeyList'], model)\n",
    "    relevant_df = feedback_df.loc[feedback_df['relevant'] == True]\n",
    "    irrelevant_df = feedback_df.loc[feedback_df['relevant'] == False]\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], relevant_df[\"summary_vec\"].values, irrelevant_df[\"summary_vec\"].values, relevant_df[\"keyList_vec\"].values, irrelevant_df[\"keyList_vec\"].values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, feedback_df, \"keyList_vec\")\n",
    "    return sorted_dataset\n",
    "\n",
    "def generate_plot():\n",
    "    rel_docs = []\n",
    "    irrel_docs = []\n",
    "    # plt.canvas.flush_events()\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['summary_vec'].values):\n",
    "        rel_docs.append(data)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == False]['summary_vec'].values):\n",
    "        irrel_docs.append(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_rel = pca.fit_transform(rel_docs)\n",
    "    pca_irrel = pca.fit_transform(irrel_docs)\n",
    "    query =np.concatenate(([np.array(query_df.iloc[0]['query_vec'])], [np.array(query_df.iloc[0]['new_query_vec'])]), axis=0)\n",
    "    pca_query = pca.fit_transform(query)\n",
    "\n",
    "    # plt.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"rel_doc\")\n",
    "    # plt.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrel_doc\")\n",
    "    # plt.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    # plt.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    # plt.legend()\n",
    "    # fileName = 'plot.png'\n",
    "    # if os.path.exists(fileName):\n",
    "    #     os.remove(fileName)\n",
    "    #\n",
    "    # plt.savefig(fileName, format='png')\n",
    "    # return plt\n",
    "    fig = Figure()\n",
    "    axis = fig.add_subplot(1, 1, 1)\n",
    "    axis.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"rel_doc\")\n",
    "    axis.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrel_doc\")\n",
    "    axis.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    axis.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    axis.legend()\n",
    "\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to extract 5 words before and after a given word\n",
    "def extract_context(text, keyword):\n",
    "    # split the text into sentences using NLTK's sent_tokenize() function\n",
    "    sentences = sent_tokenize(text)\n",
    "    # iterate through each sentence\n",
    "    context = []\n",
    "    for sentence in sentences:\n",
    "        # split the sentence into words using NLTK's word_tokenize() function\n",
    "        words = word_tokenize(sentence)\n",
    "        # check if the keyword is present in the sentence\n",
    "        if keyword in words:\n",
    "            # find the index of the keyword in the sentence\n",
    "            keyword_index = words.index(keyword)\n",
    "            # find the start and end indices for the context\n",
    "            start_index = max(0, keyword_index - 5)\n",
    "            end_index = min(len(words), keyword_index + 6)\n",
    "            # extract the context and join the words together\n",
    "            context.append(' '.join(words[start_index:end_index]))\n",
    "    # join the contexts for each sentence together\n",
    "    return ' '.join(context) if context else ''\n",
    "\n",
    "def extract_context_from_keylist(keylist, abstract):\n",
    "    result = []\n",
    "    for key in keylist:\n",
    "        result.append(extract_context(abstract, key))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cord19_df = pd.read_csv('/Users/GovindShukla/Downloads/cord19_sum_key.csv')\n",
    "#\n",
    "# model = define_d2v_model(cord19_df[~cord19_df['abstract'].isna()]['abstract'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# # Save the model as a pickle in a file\n",
    "# joblib.dump(model, 'gensim_model_2.pkl')\n",
    "# model = joblib.load('/Users/GovindShukla/Desktop/Information-Retrieval-Project/gensim_model_2.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import base64\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# app.config[\"DEBUG\"] = True\n",
    "CORS(app)\n",
    "from flask import send_file\n",
    "@app.route('/query', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('searchString')\n",
    "    print(query)\n",
    "    searchResults = search_query(query)\n",
    "    # print(searchResults.head())\n",
    "    # searchResultswithkeys = key_phrasification(searchResults)\n",
    "    print(searchResults)\n",
    "    global query_df\n",
    "    query_dict = {'query': [query], 'query_vec' : [np.nan], 'new_query_vec' : [np.nan] }\n",
    "    query_df = pd.DataFrame(data=query_dict, index=[0])\n",
    "    # print(searchResultswithkeys)\n",
    "    # searchResultswithkeys.to_csv('search_result.csv')\n",
    "    return searchResults.to_json(orient='records')\n",
    "\n",
    "# Feedback\n",
    "@app.route('/feedback', methods=['POST'])\n",
    "def fetchFeedback():\n",
    "    global reranked_df\n",
    "    feedbackJson = request.json['updates']\n",
    "    relevanceList = []\n",
    "    if len(feedbackJson):\n",
    "        for doc in feedbackJson:\n",
    "            for relevance in doc['value']:\n",
    "                relevanceList.append(relevance)\n",
    "    feedback_df = pd.DataFrame(relevanceList)\n",
    "    feedback_df.drop(columns=['bntStyle'], inplace=True)\n",
    "    reranked_df = get_reranking(feedback_df)\n",
    "    return reranked_df.to_json(orient='records')\n",
    "\n",
    "@app.route('/plot')\n",
    "def plot():\n",
    "    # Generate the plot\n",
    "\n",
    "\n",
    "    fig = generate_plot()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    output = io.BytesIO()\n",
    "    canvas.print_png(output)\n",
    "    response = Response(output.getvalue(), mimetype='image/png')\n",
    "\n",
    "    return response\n",
    "    # buffer.seek(0)\n",
    "\n",
    "    # image_png = buffer.getvalue()\n",
    "    # buffer.close()\n",
    "    # graphic = base64.b64encode(image_png).decode('utf-8')\n",
    "    # graphic = graphic.decode('utf-8')\n",
    "    # fig.savefig('plot.png')\n",
    "\n",
    "    # return send_file('plot.png', mimetype='image/png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    app.run(host='0.0.0.0',port=5117)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Feedback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_reranking(feedback_df : pd.DataFrame, model):\n",
    "    global  query_df\n",
    "    feedback_df[\"summary_vec\"] = get_vectors(feedback_df['summary'], model)\n",
    "    feedback_df[\"keyList_vec\"] = get_vectors(feedback_df['KeyList'], model)\n",
    "    relevant_df = feedback_df.loc[feedback_df['relevant'] == True]\n",
    "    irrelevant_df = feedback_df.loc[feedback_df['relevant'] == False]\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], relevant_df[\"summary_vec\"].values, irrelevant_df[\"summary_vec\"].values, relevant_df[\"keyList_vec\"].values, irrelevant_df[\"keyList_vec\"].values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, feedback_df, \"keyList_vec\")\n",
    "    return sorted_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cal_RM():\n",
    "    data = pd.read_csv('/Users/GovindShukla/Desktop/Information-Retrieval-Project/search_result.csv')\n",
    "    print(data['summary'])\n",
    "    model = define_d2v_model(data['abstract'])\n",
    "    q = {'query': ['Covid19'], 'query_vec' : [np.nan], 'new_query_vec' : [np.nan] }\n",
    "    query_df = pd.DataFrame(data=q, index=[0])\n",
    "    data[\"summary_vec\"] = get_vectors(data['summary'], model)\n",
    "    data[\"keyList_vec\"] = get_vectors(data['KeyList'], model)\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], data[\"summary_vec\"].head(3).values, data[\"summary_vec\"].head(7).values,\n",
    "                      data[\"keyList_vec\"].head(3).values, data[\"keyList_vec\"].head(7).values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, data, \"keyList_vec\")\n",
    "    print(sorted_dataset['summary'])\n",
    "cal_RM()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rel_docs = []\n",
    "irrel_docs = []\n",
    "for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['summary_vec'].values):\n",
    "    rel_docs.append(data)\n",
    "for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == False]['summary_vec'].values):\n",
    "    irrel_docs.append(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reranked_df.loc[reranked_df['relevant'] == False]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reranked_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_plot():\n",
    "    rel_docs = []\n",
    "    irrel_docs = []\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['summary_vec'].values):\n",
    "        rel_docs.append(data)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == False]['summary_vec'].values):\n",
    "        irrel_docs.append(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_rel = pca.fit_transform(rel_docs)\n",
    "    pca_irrel = pca.fit_transform(irrel_docs)\n",
    "    print(query_df)\n",
    "    query =np.concatenate(([np.array(query_df.iloc[0]['query_vec'])], [np.array(query_df.iloc[0]['new_query_vec'])]), axis=0)\n",
    "    pca_query = pca.fit_transform(query)\n",
    "\n",
    "    plt.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"rel_doc\")\n",
    "    plt.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrel_doc\")\n",
    "    plt.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    plt.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    plt.legend()\n",
    "\n",
    "    return plt\n",
    "\n",
    "generate_plot()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}