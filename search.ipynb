{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/govind17/Information-Retrieval-Project/blob/main/LDA_feature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U Flask\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install matplotlib\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# One time installation\n",
    "!pip install gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import flask\n",
    "import pandas as pd\n",
    "from flask import request\n",
    "from flask_cors import CORS\n",
    "from keyPhrasification import key_phrasification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexing the cord-19 dataset\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = 'C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/terrier_cord19'\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # create the index, using the IterDictIndexer indexer\n",
    "    indexer = pt.index.IterDictIndexer(pt_index_path)\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    index_ref = indexer.index(cord19.get_corpus_iter(),\n",
    "                              fields=('abstract',),\n",
    "                              meta=('docno',))\n",
    "else:\n",
    "    # if you already have the index, use it.\n",
    "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "    index = pt.IndexFactory.of(index_ref)\n"
   ],
   "metadata": {
    "id": "ZqTjeAaDs9aI",
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rocchio algorithm\n",
    "def rocchio_algorithm(query_doc_vector, docs_relevant_vectors,\n",
    "                      docs_irrelevant_vectors, key_relevant_vectors,\n",
    "                      key_irrelevant_vectors,\n",
    "                      alpha, beta, gamma, delta):\n",
    "\n",
    "    # sum_of_rel_doc_vectors = docs_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_doc_vectors', sum_of_rel_doc_vectors)\n",
    "    sum_of_rel_doc_vectors=[]\n",
    "    sum_of_irrel_doc_vectors=[]\n",
    "    sum_of_rel_key_vectors=[]\n",
    "    sum_of_irrel_key_vectors=[]\n",
    "    for each_rel_doc_vector in docs_relevant_vectors:\n",
    "        if len(sum_of_rel_doc_vectors) == 0:\n",
    "            print('Length of sum_of_rel_doc_vectors is zero')\n",
    "            sum_of_rel_doc_vectors = each_rel_doc_vector\n",
    "        else:\n",
    "            # print(each_rel_doc_vector)\n",
    "            sum_of_rel_doc_vectors = list(map(operator.add, sum_of_rel_doc_vectors, each_rel_doc_vector))\n",
    "            # print('SUM :', sum_of_rel_doc_vectors)\n",
    "    for each_irrel_doc_vector in docs_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_doc_vectors) == 0:\n",
    "            print('Length of sum_of_irrel_doc_vectors is zero')\n",
    "            sum_of_irrel_doc_vectors = each_irrel_doc_vector\n",
    "        else:\n",
    "            sum_of_irrel_doc_vectors = list(map(operator.add, sum_of_irrel_doc_vectors, each_irrel_doc_vector))\n",
    "    # sum_of_irrel_doc_vectors = docs_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_doc_vectors', sum_of_irrel_doc_vectors)\n",
    "    for each_rel_key_vector in key_relevant_vectors:\n",
    "        if len(sum_of_rel_key_vectors) == 0:\n",
    "            print('Length of sum_of_rel_key_vectors is zero')\n",
    "            sum_of_rel_key_vectors = each_rel_key_vector\n",
    "        else:\n",
    "            sum_of_rel_key_vectors = list(map(operator.add, sum_of_rel_key_vectors, each_rel_key_vector))\n",
    "    # sum_of_rel_key_vectors = key_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_key_vectors', sum_of_rel_key_vectors)\n",
    "    for each_irrel_key_vector in key_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_key_vectors) == 0:\n",
    "            print('Length of sum_of_irrel_key_vectors is zero')\n",
    "            sum_of_irrel_key_vectors = each_irrel_key_vector\n",
    "        else:\n",
    "            sum_of_irrel_key_vectors = list(map(operator.add, sum_of_irrel_key_vectors, each_irrel_key_vector))\n",
    "    # sum_of_irrel_key_vectors = key_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_key_vectors', (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors))\n",
    "\n",
    "    new_doc_vector_query = np.sum([np.array(query_doc_vector)\n",
    "    , (alpha/len(docs_relevant_vectors)) * np.array(sum_of_rel_doc_vectors)\n",
    "    , (beta/len(docs_irrelevant_vectors)) * np.array(sum_of_irrel_doc_vectors)\n",
    "    , (gamma/len(docs_relevant_vectors)) * np.array(sum_of_rel_key_vectors)\n",
    "    , (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors)], axis=0)\n",
    "    df = pd.DataFrame({\"a\": [new_doc_vector_query]})\n",
    "    return df.values\n",
    "\n",
    "# Compute cosine scores\n",
    "def compute_cosine_sim(new_query_vector, all_data, name):\n",
    "    consine_similarities = []\n",
    "    for index, data in all_data.iterrows():\n",
    "        cosine_sim = cosine_similarity([np.array(new_query_vector[0])], [np.array(all_data[name][index])])\n",
    "        consine_similarities.append(cosine_sim[0][0])\n",
    "    return consine_similarities\n",
    "\n",
    "# Rank data\n",
    "def rank_data(new_query_vector, dataset, name):\n",
    "  cosine_sim_values = compute_cosine_sim(new_query_vector, dataset, name)\n",
    "  dataset['Cosine_Similarity_' + name] = cosine_sim_values\n",
    "  sorted_dataset = dataset.sort_values(by=['Cosine_Similarity_' + name], ascending=False)\n",
    "\n",
    "  return sorted_dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "def pre_processing_data(data):\n",
    "    ps = PorterStemmer()\n",
    "    tagged_dataset = [TaggedDocument(words=[ps.stem(w) for w in nltk.word_tokenize(_d) if word_tokenize(_d.lower()) not in stopwords.words('english')], tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    #tagged_dataset = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    return tagged_dataset\n",
    "\n",
    "def model_doc2vec(model, tagged_data, num_epochs):\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,total_examples=len(tagged_data), epochs=num_epochs)\n",
    "    return model\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "def define_d2v_model(dataset):\n",
    "    preprocessed_tagged_dataset = pre_processing_data(dataset)\n",
    "    model_d2v = Doc2Vec(dm=1, vector_size=100, window = 10, negative=5, hs=0, min_count=2, sample = 0, alpha=0.025, min_alpha=0.001, dm_mean = 0, dbow_words=1)\n",
    "    model = model_doc2vec(model_d2v, preprocessed_tagged_dataset, 100)\n",
    "    return model\n",
    "\n",
    "def get_vectors(input_data, model):\n",
    "    preprocessed_tagged_input_data= pre_processing_data(input_data)\n",
    "    id, vectors = np.array(vector_for_learning(model, preprocessed_tagged_input_data), dtype=object)\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pritha\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "monoT5:   0%|          | 0/3 [00:00<?, ?batches/s]Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "monoT5: 100%|██████████| 3/3 [00:12<00:00,  4.19s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['docno', 'title', 'abstract_x', 'summary', 'key_phrases', 'qid',\n",
      "       'docid', 'query', 'abstract_y', 'score', 'rank'],\n",
      "      dtype='object')\n",
      "Index(['docno', 'title', 'abstract', 'summary', 'KeyList'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      docno                                              title  \\\n",
      "0  awitk3se  COVID-19 (Novel Coronavirus 2019) - recent tre...   \n",
      "1  s4dqx9en  Plasma Metabolomic and Lipidomic Alterations A...   \n",
      "2  u56ydlve  The impact of believing you have had COVID-19 ...   \n",
      "3  3kmxzbm7  COVID-19 (Novel Coronavirus 2019) - recent trends   \n",
      "4  l2rmmjqb  Validation of the British Society of Thoracic ...   \n",
      "5  zub7xdi9  Comparison of the computed tomography findings...   \n",
      "6  rxd08ouk  Cognitive, Affective, and Behavioral Construct...   \n",
      "7  448tamvr  Validation of the British Society of Thoracic ...   \n",
      "8  eyelcflh  Cognitive, Affective, and Behavioral Construct...   \n",
      "9  1loqavom  Stability Analysis and Numerical Simulation of...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  The World Health Organization (WHO) has issued...   \n",
      "1  The pandemic of the coronavirus disease 2019 (...   \n",
      "2  Objectives: To investigate whether people who ...   \n",
      "3  The World Health Organization (WHO) has issued...   \n",
      "4  AIM: To validate the British Society of Thorac...   \n",
      "5  OBJECTIVES: To compare the chest computed tomo...   \n",
      "6  This online survey study aimed to compare the ...   \n",
      "7  Abstract Aim To validate the British Society o...   \n",
      "8  This online survey study aimed to compare the ...   \n",
      "9  The Aim of this research is construct the SEIR...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  COVID-19 is similar to Severe Acute Respirator...   \n",
      "1  COVID-19 is marked by its rapid progression fr...   \n",
      "2  The impact of believing you have had COVID-19 ...   \n",
      "3  COVID-19 is similar to Severe Acute Respirator...   \n",
      "4  Validation of the British Society of Thoracic ...   \n",
      "5  Comparison of the computed tomography findings...   \n",
      "6  In total, 533 sexual minority and 1421 heteros...   \n",
      "7  Validation of the British Society of Thoracic ...   \n",
      "8  In total, 533 sexual minority and 1421 heteros...   \n",
      "9  Stability Analysis and Numerical Simulation of...   \n",
      "\n",
      "                                             KeyList  \n",
      "0  [covid-19, sars-cov, similar, severe acute res...  \n",
      "1  [covid-19, lipid alterations, severe condition...  \n",
      "2  [covid-19, people, social distancing measures,...  \n",
      "3  [covid-19, sars-cov, similar, severe acute res...  \n",
      "4  [covid-19, patients, covid-19 chest radiograph...  \n",
      "5  [covid-19, ggo, bilateral distribution, lower ...  \n",
      "6  [covid-19, sexual minority, heterosexual indiv...  \n",
      "7  [covid-19, patients, covid-19 chest radiograph...  \n",
      "8  [covid-19, sexual minority, heterosexual indiv...  \n",
      "9  [pandemic covid-19, seir model, stability anal...  \n"
     ]
    },
    {
     "data": {
      "text/plain": "      docno                                              title  \\\n0  awitk3se  COVID-19 (Novel Coronavirus 2019) - recent tre...   \n1  s4dqx9en  Plasma Metabolomic and Lipidomic Alterations A...   \n2  u56ydlve  The impact of believing you have had COVID-19 ...   \n3  3kmxzbm7  COVID-19 (Novel Coronavirus 2019) - recent trends   \n4  l2rmmjqb  Validation of the British Society of Thoracic ...   \n5  zub7xdi9  Comparison of the computed tomography findings...   \n6  rxd08ouk  Cognitive, Affective, and Behavioral Construct...   \n7  448tamvr  Validation of the British Society of Thoracic ...   \n8  eyelcflh  Cognitive, Affective, and Behavioral Construct...   \n9  1loqavom  Stability Analysis and Numerical Simulation of...   \n\n                                          abstract_x  \\\n0  The World Health Organization (WHO) has issued...   \n1  The pandemic of the coronavirus disease 2019 (...   \n2  Objectives: To investigate whether people who ...   \n3  The World Health Organization (WHO) has issued...   \n4  AIM: To validate the British Society of Thorac...   \n5  OBJECTIVES: To compare the chest computed tomo...   \n6  This online survey study aimed to compare the ...   \n7  Abstract Aim To validate the British Society o...   \n8  This online survey study aimed to compare the ...   \n9  The Aim of this research is construct the SEIR...   \n\n                                             summary  key_phrases qid   docid  \\\n0  COVID-19 is similar to Severe Acute Respirator...          NaN   1   32226   \n1  COVID-19 is marked by its rapid progression fr...          NaN   1   68320   \n2  The impact of believing you have had COVID-19 ...          NaN   1   75497   \n3  COVID-19 is similar to Severe Acute Respirator...          NaN   1   93863   \n4  Validation of the British Society of Thoracic ...          NaN   1  109749   \n5  Comparison of the computed tomography findings...          NaN   1  112383   \n6  In total, 533 sexual minority and 1421 heteros...          NaN   1  126243   \n7  Validation of the British Society of Thoracic ...          NaN   1  150199   \n8  In total, 533 sexual minority and 1421 heteros...          NaN   1  153035   \n9  Stability Analysis and Numerical Simulation of...          NaN   1  171671   \n\n   query                                         abstract_y     score  rank  \n0  covid  The World Health Organization (WHO) has issued... -0.161107     3  \n1  covid  The pandemic of the coronavirus disease 2019 (... -0.267529     5  \n2  covid  Objectives: To investigate whether people who ... -0.856843     9  \n3  covid  The World Health Organization (WHO) has issued... -0.161107     4  \n4  covid  AIM: To validate the British Society of Thorac... -0.333039     6  \n5  covid  OBJECTIVES: To compare the chest computed tomo... -0.085978     0  \n6  covid  This online survey study aimed to compare the ... -0.112808     1  \n7  covid  Abstract Aim To validate the British Society o... -0.480262     7  \n8  covid  This online survey study aimed to compare the ... -0.112808     2  \n9  covid  The Aim of this research is construct the SEIR... -0.639952     8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>docno</th>\n      <th>title</th>\n      <th>abstract_x</th>\n      <th>summary</th>\n      <th>key_phrases</th>\n      <th>qid</th>\n      <th>docid</th>\n      <th>query</th>\n      <th>abstract_y</th>\n      <th>score</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>awitk3se</td>\n      <td>COVID-19 (Novel Coronavirus 2019) - recent tre...</td>\n      <td>The World Health Organization (WHO) has issued...</td>\n      <td>COVID-19 is similar to Severe Acute Respirator...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>32226</td>\n      <td>covid</td>\n      <td>The World Health Organization (WHO) has issued...</td>\n      <td>-0.161107</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>s4dqx9en</td>\n      <td>Plasma Metabolomic and Lipidomic Alterations A...</td>\n      <td>The pandemic of the coronavirus disease 2019 (...</td>\n      <td>COVID-19 is marked by its rapid progression fr...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>68320</td>\n      <td>covid</td>\n      <td>The pandemic of the coronavirus disease 2019 (...</td>\n      <td>-0.267529</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>u56ydlve</td>\n      <td>The impact of believing you have had COVID-19 ...</td>\n      <td>Objectives: To investigate whether people who ...</td>\n      <td>The impact of believing you have had COVID-19 ...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>75497</td>\n      <td>covid</td>\n      <td>Objectives: To investigate whether people who ...</td>\n      <td>-0.856843</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3kmxzbm7</td>\n      <td>COVID-19 (Novel Coronavirus 2019) - recent trends</td>\n      <td>The World Health Organization (WHO) has issued...</td>\n      <td>COVID-19 is similar to Severe Acute Respirator...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>93863</td>\n      <td>covid</td>\n      <td>The World Health Organization (WHO) has issued...</td>\n      <td>-0.161107</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>l2rmmjqb</td>\n      <td>Validation of the British Society of Thoracic ...</td>\n      <td>AIM: To validate the British Society of Thorac...</td>\n      <td>Validation of the British Society of Thoracic ...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>109749</td>\n      <td>covid</td>\n      <td>AIM: To validate the British Society of Thorac...</td>\n      <td>-0.333039</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>zub7xdi9</td>\n      <td>Comparison of the computed tomography findings...</td>\n      <td>OBJECTIVES: To compare the chest computed tomo...</td>\n      <td>Comparison of the computed tomography findings...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>112383</td>\n      <td>covid</td>\n      <td>OBJECTIVES: To compare the chest computed tomo...</td>\n      <td>-0.085978</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>rxd08ouk</td>\n      <td>Cognitive, Affective, and Behavioral Construct...</td>\n      <td>This online survey study aimed to compare the ...</td>\n      <td>In total, 533 sexual minority and 1421 heteros...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>126243</td>\n      <td>covid</td>\n      <td>This online survey study aimed to compare the ...</td>\n      <td>-0.112808</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>448tamvr</td>\n      <td>Validation of the British Society of Thoracic ...</td>\n      <td>Abstract Aim To validate the British Society o...</td>\n      <td>Validation of the British Society of Thoracic ...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>150199</td>\n      <td>covid</td>\n      <td>Abstract Aim To validate the British Society o...</td>\n      <td>-0.480262</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>eyelcflh</td>\n      <td>Cognitive, Affective, and Behavioral Construct...</td>\n      <td>This online survey study aimed to compare the ...</td>\n      <td>In total, 533 sexual minority and 1421 heteros...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>153035</td>\n      <td>covid</td>\n      <td>This online survey study aimed to compare the ...</td>\n      <td>-0.112808</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1loqavom</td>\n      <td>Stability Analysis and Numerical Simulation of...</td>\n      <td>The Aim of this research is construct the SEIR...</td>\n      <td>Stability Analysis and Numerical Simulation of...</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>171671</td>\n      <td>covid</td>\n      <td>The Aim of this research is construct the SEIR...</td>\n      <td>-0.639952</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "import pandas as pd\n",
    "from keyPhrasification import key_phrasification\n",
    "\n",
    "monoT5 = MonoT5ReRanker(text_field='abstract')\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "data = pd.read_csv('/Users/GovindShukla/Desktop/Information-Retrieval-Project/search_result.csv')\n",
    "model = define_d2v_model(data['abstract'])\n",
    "\n",
    "def search_query(query):\n",
    "  index_ref2 = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "  index2 = pt.IndexFactory.of(index_ref2)\n",
    "  # print(query)\n",
    "  if not pt.started():\n",
    "      pt.init()\n",
    "  br = pt.BatchRetrieve(index2) % 10\n",
    "  pipeline = (br >> pt.text.get_text(dataset, 'abstract') >> monoT5)\n",
    "  search_result = pipeline.search(query)\n",
    "  # print(search_result)\n",
    "  cord19_docs = pd.read_csv('/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/cord19_sum.csv')\n",
    "  filtered_docs = pd.merge(cord19_docs, search_result, on = \"docno\", how = \"inner\")\n",
    "  searchResultswithkeys = key_phrasification(filtered_docs)\n",
    "  print(searchResultswithkeys)\n",
    "  return filtered_docs\n",
    "\n",
    "search_query('covid')"
   ],
   "metadata": {
    "id": "K1L5hBv_s9aP",
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflask\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m request\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflask_cors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CORS\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeyPhrasification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m key_phrasification\n\u001B[1;32m      7\u001B[0m app \u001B[38;5;241m=\u001B[39m flask\u001B[38;5;241m.\u001B[39mFlask(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# app.config[\"DEBUG\"] = True\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Information-Retrieval-Project/keyPhrasification.py:2\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpke\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcsv\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/site-packages/pke/__init__.py:4\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpke\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_structures\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Candidate, Sentence\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpke\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LoadFile\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpke\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      6\u001B[0m     load_document_frequency_file, compute_document_frequency,\n\u001B[1;32m      7\u001B[0m     train_supervised_model, load_references,\n\u001B[1;32m      8\u001B[0m     compute_lda_model, load_lda_model)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpke\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01munsupervised\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/site-packages/pke/base.py:13\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RegexpParser\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msnowball\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SnowballStemmer\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpke\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlang\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stopwords, langcodes\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstring\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m punctuation\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/site-packages/pke/lang.py:37\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m langcode \u001B[38;5;129;01min\u001B[39;00m langcodes:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 37\u001B[0m         tmp \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mspacy.lang.\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlangcode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m         stopwords[langcode] \u001B[38;5;241m=\u001B[39m tmp\u001B[38;5;241m.\u001B[39mstop_words\u001B[38;5;241m.\u001B[39mSTOP_WORDS\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/site-packages/spacy/lang/fr/__init__.py:5\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Optional, Callable\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenizer_exceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOKENIZER_EXCEPTIONS, TOKEN_MATCH\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpunctuation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOKENIZER_PREFIXES, TOKENIZER_INFIXES\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpunctuation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOKENIZER_SUFFIXES\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/site-packages/spacy/lang/fr/tokenizer_exceptions.py:442\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    433\u001B[0m _regular_exp \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^[\u001B[39m\u001B[38;5;132;01m{a}\u001B[39;00m\u001B[38;5;124m]+[\u001B[39m\u001B[38;5;132;01m{hyphen}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;132;01m{hyphen_combo}\u001B[39;00m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{hyphen}\u001B[39;00m\u001B[38;5;124m](?:l[\u001B[39m\u001B[38;5;132;01m{elision}\u001B[39;00m\u001B[38;5;124m])?[\u001B[39m\u001B[38;5;132;01m{a}\u001B[39;00m\u001B[38;5;124m]+$\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    435\u001B[0m         hyphen_combo\u001B[38;5;241m=\u001B[39mhc, elision\u001B[38;5;241m=\u001B[39mELISION, hyphen\u001B[38;5;241m=\u001B[39mHYPHENS, a\u001B[38;5;241m=\u001B[39mALPHA\n\u001B[1;32m    436\u001B[0m     )\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hc \u001B[38;5;129;01min\u001B[39;00m _hyphen_combination\n\u001B[1;32m    438\u001B[0m ]\n\u001B[1;32m    441\u001B[0m TOKENIZER_EXCEPTIONS \u001B[38;5;241m=\u001B[39m update_exc(BASE_EXCEPTIONS, _exc)\n\u001B[0;32m--> 442\u001B[0m TOKEN_MATCH \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m(?iu)\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m|\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m(?:\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_regular_exp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    444\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmatch\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/re.py:251\u001B[0m, in \u001B[0;36mcompile\u001B[0;34m(pattern, flags)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompile\u001B[39m(pattern, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompile a regular expression pattern, returning a Pattern object.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/re.py:303\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(pattern, flags)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sre_compile\u001B[38;5;241m.\u001B[39misstring(pattern):\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst argument must be string or compiled pattern\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 303\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43msre_compile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (flags \u001B[38;5;241m&\u001B[39m DEBUG):\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(_cache) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m _MAXCACHE:\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;66;03m# Drop the oldest item\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/sre_compile.py:768\u001B[0m, in \u001B[0;36mcompile\u001B[0;34m(p, flags)\u001B[0m\n\u001B[1;32m    765\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    766\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 768\u001B[0m code \u001B[38;5;241m=\u001B[39m \u001B[43m_code\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m&\u001B[39m SRE_FLAG_DEBUG:\n\u001B[1;32m    771\u001B[0m     \u001B[38;5;28mprint\u001B[39m()\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/sre_compile.py:607\u001B[0m, in \u001B[0;36m_code\u001B[0;34m(p, flags)\u001B[0m\n\u001B[1;32m    604\u001B[0m _compile_info(code, p, flags)\n\u001B[1;32m    606\u001B[0m \u001B[38;5;66;03m# compile the pattern\u001B[39;00m\n\u001B[0;32m--> 607\u001B[0m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m code\u001B[38;5;241m.\u001B[39mappend(SUCCESS)\n\u001B[1;32m    611\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m code\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/sre_compile.py:209\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(code, pattern, flags)\u001B[0m\n\u001B[1;32m    207\u001B[0m skip \u001B[38;5;241m=\u001B[39m _len(code); emit(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    208\u001B[0m \u001B[38;5;66;03m# _compile_info(code, av, flags)\u001B[39;00m\n\u001B[0;32m--> 209\u001B[0m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mav\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m emit(JUMP)\n\u001B[1;32m    211\u001B[0m tailappend(_len(code)); emit(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/sre_compile.py:148\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(code, pattern, flags)\u001B[0m\n\u001B[1;32m    146\u001B[0m emit(av[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    147\u001B[0m emit(av[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 148\u001B[0m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mav\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m emit(SUCCESS)\n\u001B[1;32m    150\u001B[0m code[skip] \u001B[38;5;241m=\u001B[39m _len(code) \u001B[38;5;241m-\u001B[39m skip\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/sre_compile.py:120\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(code, pattern, flags)\u001B[0m\n\u001B[1;32m    118\u001B[0m             code[skip] \u001B[38;5;241m=\u001B[39m _len(code) \u001B[38;5;241m-\u001B[39m skip\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m op \u001B[38;5;129;01mis\u001B[39;00m IN:\n\u001B[0;32m--> 120\u001B[0m     charset, hascased \u001B[38;5;241m=\u001B[39m \u001B[43m_optimize_charset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mav\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miscased\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtolower\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m&\u001B[39m SRE_FLAG_IGNORECASE \u001B[38;5;129;01mand\u001B[39;00m flags \u001B[38;5;241m&\u001B[39m SRE_FLAG_LOCALE:\n\u001B[1;32m    122\u001B[0m         emit(IN_LOC_IGNORE)\n",
      "File \u001B[0;32m~/miniforge3/envs/GSCS/lib/python3.10/sre_compile.py:302\u001B[0m, in \u001B[0;36m_optimize_charset\u001B[0;34m(charset, iscased, fixup, fixes)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mmap\u001B[39m(fixup, r):\n\u001B[1;32m    301\u001B[0m     charmap[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m fixes:\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m fixes[i]:\n\u001B[1;32m    304\u001B[0m             charmap[k] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# app.config[\"DEBUG\"] = True\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/query', methods=['GET'])\n",
    "def search():\n",
    "    # query = request.args.get('searchString')\n",
    "    # print(query)\n",
    "    #searchResults = search_query(query)\n",
    "    # print(searchResults.head())\n",
    "    #searchResultswithkeys = key_phrasification(searchResults)\n",
    "    searchResultswithkeys = pd.read_csv('/Users/GovindShukla/Desktop/Information-Retrieval-Project/search_result.csv')\n",
    "    print(searchResultswithkeys)\n",
    "    return searchResultswithkeys.to_json(orient='records')\n",
    "\n",
    "\n",
    "@app.route('/feedback', methods=['POST'])\n",
    "def fetchFeedback():\n",
    "    # corpus_df = pd.read_csv(\n",
    "    #     '/Users/GovindShukla/Desktop/Information-Retrieval-Project/RankedDocuments/trec_docs_sample.csv')\n",
    "    # corpus = corpus_df['text'].values\n",
    "    # list = request.args.get('feedbackList')\n",
    "    # # print(request.body)\n",
    "    # print(list)\n",
    "    # return\n",
    "    feedbackJson = request.json['updates']\n",
    "    relevanceList = []\n",
    "    if len(feedbackJson):\n",
    "        for doc in feedbackJson:\n",
    "            for relevance in doc['value']:\n",
    "                relevanceList.append(relevance)\n",
    "    feedback_df = pd.DataFrame(relevanceList)\n",
    "    feedback_df.drop(columns=['bntStyle'])\n",
    "\n",
    "    return relevanceList\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     corpus_df = pd.read_csv(\n",
    "#         '/Users/GovindShukla/Desktop/Information-Retrieval-Project/RankedDocuments/trec_docs_sample.csv')\n",
    "#     corpus = corpus_df['text'].values\n",
    "if __name__ == \"__main__\":\n",
    "    # app.debug = True\n",
    "    app.run(host='0.0.0.0',port=5022)\n",
    "#search_result.csv\n",
    "#     from flask import Flask\n",
    "# app = Flask(__name__)\n",
    "#\n",
    "# @app.route(\"/\")\n",
    "# def hello(): return \"Hello World\"\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#   app.debug = True\n",
    "#   app.run(host='0.0.0.0',port=5005)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Feedback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    COVID-19 is similar to Severe Acute Respirator...\n",
      "1    COVID-19 is marked by its rapid progression fr...\n",
      "2    The impact of believing you have had COVID-19 ...\n",
      "3    COVID-19 is similar to Severe Acute Respirator...\n",
      "4    Validation of the British Society of Thoracic ...\n",
      "5    Comparison of the computed tomography findings...\n",
      "6    In total, 533 sexual minority and 1421 heteros...\n",
      "7    Validation of the British Society of Thoracic ...\n",
      "8    In total, 533 sexual minority and 1421 heteros...\n",
      "9    Stability Analysis and Numerical Simulation of...\n",
      "Name: summary, dtype: object\n",
      "Length of sum_of_rel_doc_vectors is zero\n",
      "Length of sum_of_irrel_doc_vectors is zero\n",
      "Length of sum_of_rel_key_vectors is zero\n",
      "Length of sum_of_irrel_key_vectors is zero\n",
      "1    COVID-19 is marked by its rapid progression fr...\n",
      "0    COVID-19 is similar to Severe Acute Respirator...\n",
      "5    Comparison of the computed tomography findings...\n",
      "3    COVID-19 is similar to Severe Acute Respirator...\n",
      "2    The impact of believing you have had COVID-19 ...\n",
      "9    Stability Analysis and Numerical Simulation of...\n",
      "8    In total, 533 sexual minority and 1421 heteros...\n",
      "6    In total, 533 sexual minority and 1421 heteros...\n",
      "7    Validation of the British Society of Thoracic ...\n",
      "4    Validation of the British Society of Thoracic ...\n",
      "Name: summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def cal_RM():\n",
    "    data = pd.read_csv('/Users/GovindShukla/Desktop/Information-Retrieval-Project/search_result.csv')\n",
    "    print(data['summary'])\n",
    "    model = define_d2v_model(data['abstract'])\n",
    "    q = {'query': ['Covid19 Origin'], 'query_vec' : [np.nan], 'new_query_vec' : [np.nan] }\n",
    "    query_df = pd.DataFrame(data=q, index=[0])\n",
    "    data[\"summary_vec\"] = get_vectors(data['summary'], model)\n",
    "    data[\"keyList_vec\"] = get_vectors(data['KeyList'], model)\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], data[\"summary_vec\"].head(3).values, data[\"summary_vec\"].head(7).values,\n",
    "                      data[\"keyList_vec\"].head(3).values, data[\"keyList_vec\"].head(7).values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, data, \"keyList_vec\")\n",
    "    print(sorted_dataset['summary'])\n",
    "cal_RM()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            query                                          query_vec\n",
      "0  Covid19 Origin  [0.004971251, 0.004715933, -0.0032024188, 0.00...\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0.004971251, 0.004715933, -0.0032024188, 0.00...\n",
      "Name: query_vec, dtype: object\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}