{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/govind17/Information-Retrieval-Project/blob/main/LDA_feature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U Flask\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install matplotlib\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pritha\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Pritha\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\Pritha\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pritha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pritha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One time installation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import io\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import flask\n",
    "from flask import request\n",
    "from flask_cors import CORS\n",
    "from flask import Flask, Response\n",
    "from keyPhrasification import key_phrasification\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pyterrier as pt\n",
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import ast\n",
    "from keyPhrasification import key_phrasification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexing the cord-19 dataset\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = 'C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/terrier_cord19'\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # create the index, using the IterDictIndexer indexer\n",
    "    indexer = pt.index.IterDictIndexer(pt_index_path)\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    index_ref = indexer.index(cord19.get_corpus_iter(),\n",
    "                              fields=('abstract',),\n",
    "                              meta=('docno',))\n",
    "else:\n",
    "    # if you already have the index, use it.\n",
    "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "    index = pt.IndexFactory.of(index_ref)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Rocchio algorithm\n",
    "def rocchio_algorithm(query_doc_vector, docs_relevant_vectors,\n",
    "                      docs_irrelevant_vectors, key_relevant_vectors,\n",
    "                      key_irrelevant_vectors,\n",
    "                      alpha, beta, gamma, delta):\n",
    "\n",
    "    # sum_of_rel_doc_vectors = docs_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_doc_vectors', sum_of_rel_doc_vectors)\n",
    "    sum_of_rel_doc_vectors=[]\n",
    "    sum_of_irrel_doc_vectors=[]\n",
    "    sum_of_rel_key_vectors=[]\n",
    "    sum_of_irrel_key_vectors=[]\n",
    "    for each_rel_doc_vector in docs_relevant_vectors:\n",
    "        if len(sum_of_rel_doc_vectors) == 0:\n",
    "            print('Length of sum_of_rel_doc_vectors is zero')\n",
    "            sum_of_rel_doc_vectors = each_rel_doc_vector\n",
    "        else:\n",
    "            # print(each_rel_doc_vector)\n",
    "            sum_of_rel_doc_vectors = list(map(operator.add, sum_of_rel_doc_vectors, each_rel_doc_vector))\n",
    "            # print('SUM :', sum_of_rel_doc_vectors)\n",
    "    for each_irrel_doc_vector in docs_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_doc_vectors) == 0:\n",
    "            print('Length of sum_of_irrel_doc_vectors is zero')\n",
    "            sum_of_irrel_doc_vectors = each_irrel_doc_vector\n",
    "        else:\n",
    "            sum_of_irrel_doc_vectors = list(map(operator.add, sum_of_irrel_doc_vectors, each_irrel_doc_vector))\n",
    "    # sum_of_irrel_doc_vectors = docs_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_doc_vectors', sum_of_irrel_doc_vectors)\n",
    "    for each_rel_key_vector in key_relevant_vectors:\n",
    "        if len(sum_of_rel_key_vectors) == 0:\n",
    "            print('Length of sum_of_rel_key_vectors is zero')\n",
    "            sum_of_rel_key_vectors = each_rel_key_vector\n",
    "        else:\n",
    "            sum_of_rel_key_vectors = list(map(operator.add, sum_of_rel_key_vectors, each_rel_key_vector))\n",
    "    # sum_of_rel_key_vectors = key_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_key_vectors', sum_of_rel_key_vectors)\n",
    "    for each_irrel_key_vector in key_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_key_vectors) == 0:\n",
    "            print('Length of sum_of_irrel_key_vectors is zero')\n",
    "            sum_of_irrel_key_vectors = each_irrel_key_vector\n",
    "        else:\n",
    "            sum_of_irrel_key_vectors = list(map(operator.add, sum_of_irrel_key_vectors, each_irrel_key_vector))\n",
    "    # sum_of_irrel_key_vectors = key_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_key_vectors', (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors))\n",
    "\n",
    "    new_doc_vector_query = np.sum([np.array(query_doc_vector)\n",
    "    , (alpha/len(docs_relevant_vectors)) * np.array(sum_of_rel_doc_vectors)\n",
    "    , (beta/len(docs_irrelevant_vectors)) * np.array(sum_of_irrel_doc_vectors)\n",
    "    , (gamma/len(docs_relevant_vectors)) * np.array(sum_of_rel_key_vectors)\n",
    "    , (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors)], axis=0)\n",
    "    df = pd.DataFrame({\"a\": [new_doc_vector_query]})\n",
    "    return df.values\n",
    "\n",
    "# Compute cosine scores\n",
    "def compute_cosine_sim(new_query_vector, all_data, name):\n",
    "    consine_similarities = []\n",
    "    for index, data in all_data.iterrows():\n",
    "        cosine_sim = cosine_similarity([np.array(new_query_vector[0])], [np.array(all_data[name][index])])\n",
    "        consine_similarities.append(cosine_sim[0][0])\n",
    "    return consine_similarities\n",
    "\n",
    "# Rank data\n",
    "def rank_data(new_query_vector, dataset, name):\n",
    "  cosine_sim_values = compute_cosine_sim(new_query_vector, dataset, name)\n",
    "  dataset['Cosine_Similarity_' + name] = cosine_sim_values\n",
    "  sorted_dataset = dataset.sort_values(by=['Cosine_Similarity_' + name], ascending=False)\n",
    "\n",
    "  return sorted_dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "def pre_processing_data(data):\n",
    "    ps = PorterStemmer()\n",
    "    tagged_dataset = [TaggedDocument(words=[ps.stem(w) for w in nltk.word_tokenize(str(_d)) if word_tokenize(str(_d).lower()) not in stopwords.words('english')], tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    #tagged_dataset = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    return tagged_dataset\n",
    "\n",
    "def model_doc2vec(model, tagged_data, num_epochs):\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,total_examples=len(tagged_data), epochs=num_epochs)\n",
    "    return model\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "def define_d2v_model(dataset):\n",
    "    preprocessed_tagged_dataset = pre_processing_data(dataset)\n",
    "    model_d2v = Doc2Vec(dm=1, vector_size=100, window = 10, negative=5, hs=0, min_count=2, sample = 0, alpha=0.025, min_alpha=0.001, dm_mean = 0, dbow_words=1)\n",
    "    model = model_doc2vec(model_d2v, preprocessed_tagged_dataset, 100)\n",
    "    return model\n",
    "\n",
    "def get_vectors(input_data, model):\n",
    "    preprocessed_tagged_input_data= pre_processing_data(input_data)\n",
    "    id, vectors = np.array(vector_for_learning(model, preprocessed_tagged_input_data), dtype=object)\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pritha\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "monoT5 = MonoT5ReRanker(text_field='abstract')\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "cord19_df = pd.read_csv('C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/cord19_sum.csv')\n",
    "# model = define_d2v_model(cord19_df['abstract'])\n",
    "model = joblib.load('gensim_model_2.pkl')\n",
    "query_df = pd.DataFrame\n",
    "reranked_df = pd.DataFrame\n",
    "\n",
    "def search_query(query):\n",
    "  index_ref2 = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "  index2 = pt.IndexFactory.of(index_ref2)\n",
    "  # print(query)\n",
    "  if not pt.started():\n",
    "      pt.init()\n",
    "  br = pt.BatchRetrieve(index2) % 15\n",
    "  pipeline = (br >> pt.text.get_text(dataset, 'abstract') >> monoT5)\n",
    "  search_result = pipeline.search(query)\n",
    "  # search_result = search_result.drop_duplicates(subset=['summary'])\n",
    "  print(search_result)\n",
    "  filtered_docs = pd.merge(cord19_df, search_result, on = \"docno\", how = \"inner\")\n",
    "\n",
    "  filtered_docs = filtered_docs.drop_duplicates(subset=['docno', 'summary'])\n",
    "  return filtered_docs.head(10)\n",
    "  # return pd.read_csv('search_result.csv')\n",
    "\n",
    "\n",
    "# search_query('covid')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_reranking(feedback_df : pd.DataFrame):\n",
    "    global  query_df\n",
    "    global reranked_df\n",
    "    feedback_df[\"summary_vec\"] = get_vectors(feedback_df['summary'], model)\n",
    "    feedback_df[\"keyList_vec\"] = get_vectors(feedback_df['KeyList'], model)\n",
    "    relevant_df = feedback_df.loc[feedback_df['relevant'] == True]\n",
    "    irrelevant_df = feedback_df.loc[feedback_df['relevant'] == False]\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], relevant_df[\"summary_vec\"].values, irrelevant_df[\"summary_vec\"].values, relevant_df[\"keyList_vec\"].values, irrelevant_df[\"keyList_vec\"].values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, feedback_df, \"keyList_vec\")\n",
    "    return sorted_dataset\n",
    "\n",
    "def generate_plot():\n",
    "    rel_docs = []\n",
    "    irrel_docs = []\n",
    "    # plt.canvas.flush_events()\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['summary_vec'].values):\n",
    "        rel_docs.append(data)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == False]['summary_vec'].values):\n",
    "        irrel_docs.append(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_rel = pca.fit_transform(rel_docs)\n",
    "    pca_irrel = pca.fit_transform(irrel_docs)\n",
    "    query =np.concatenate(([np.array(query_df.iloc[0]['query_vec'])], [np.array(query_df.iloc[0]['new_query_vec'])]), axis=0)\n",
    "    pca_query = pca.fit_transform(query)\n",
    "\n",
    "    # plt.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"rel_doc\")\n",
    "    # plt.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrel_doc\")\n",
    "    # plt.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    # plt.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    # plt.legend()\n",
    "    # fileName = 'plot.png'\n",
    "    # if os.path.exists(fileName):\n",
    "    #     os.remove(fileName)\n",
    "    #\n",
    "    # plt.savefig(fileName, format='png')\n",
    "    # return plt\n",
    "    fig = Figure()\n",
    "    axis = fig.add_subplot(1, 1, 1)\n",
    "    axis.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"rel_doc\")\n",
    "    axis.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrel_doc\")\n",
    "    axis.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    axis.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    axis.legend()\n",
    "\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# function to extract 5 words before and after a given word\n",
    "def extract_context(text, keyword):\n",
    "    # split the text into sentences using NLTK's sent_tokenize() function\n",
    "    sentences = sent_tokenize(text)\n",
    "    # iterate through each sentence\n",
    "    context = []\n",
    "    for sentence in sentences:\n",
    "        # split the sentence into words using NLTK's word_tokenize() function\n",
    "        words = word_tokenize(sentence)\n",
    "        # check if the keyword is present in the sentence\n",
    "        if keyword in words:\n",
    "            # find the index of the keyword in the sentence\n",
    "            keyword_index = words.index(keyword)\n",
    "            # find the start and end indices for the context\n",
    "            start_index = max(0, keyword_index - 5)\n",
    "            end_index = min(len(words), keyword_index + 6)\n",
    "            # extract the context and join the words together\n",
    "            context.append(' '.join(words[start_index:end_index]))\n",
    "    # join the contexts for each sentence together\n",
    "    return ' '.join(context) if context else ''\n",
    "\n",
    "def extract_context_from_keylist(keylist, abstract):\n",
    "    result = []\n",
    "    for key in keylist:\n",
    "        result.append(extract_context(abstract, key))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cord19_df = pd.read_csv('/Users/GovindShukla/Downloads/cord19_sum_key.csv')\n",
    "#\n",
    "# model = define_d2v_model(cord19_df[~cord19_df['abstract'].isna()]['abstract'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# # Save the model as a pickle in a file\n",
    "# joblib.dump(model, 'gensim_model_2.pkl')\n",
    "# model = joblib.load('/Users/GovindShukla/Desktop/Information-Retrieval-Project/gensim_model_2.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5132\n",
      " * Running on http://192.168.0.15:5132\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandemic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|██████████| 4/4 [00:17<00:00,  4.41s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid   docid     docno     query  \\\n",
      "4    1   52415  8n0mwad5  pandemic   \n",
      "10   1  161087  9s7y8guc  pandemic   \n",
      "14   1  134190  p94jups3  pandemic   \n",
      "0    1     513  dzzlpz4o  pandemic   \n",
      "12   1   61346  v932f5ni  pandemic   \n",
      "9    1  177207  5nkrrqqw  pandemic   \n",
      "11   1   40347  4upkfpuz  pandemic   \n",
      "6    1  149026  onyoh6lo  pandemic   \n",
      "2    1  111087  1c2rnwjb  pandemic   \n",
      "3    1  111088  e171iero  pandemic   \n",
      "5    1  192498  08gqn86z  pandemic   \n",
      "1    1   48139  313bi7q4  pandemic   \n",
      "7    1   69605  wp9dpiqs  pandemic   \n",
      "13   1   88814  n80jy8zl  pandemic   \n",
      "8    1  179379  wp9dpiqs  pandemic   \n",
      "\n",
      "                                             abstract     score  rank  \n",
      "4   The 1918 to 1919 H1N1 influenza pandemic is am... -0.711614     0  \n",
      "10  Summary It is commonly believed that the clini... -1.087005     1  \n",
      "14  Few viruses have shaped the course of human hi... -1.275363     2  \n",
      "0   BACKGROUND: More than a year after an influenz... -1.449998     3  \n",
      "12  Influenza pandemics have occurred throughout h... -1.786306     4  \n",
      "9   Summary The 2009 H1N1 influenza pandemic was a... -1.940994     5  \n",
      "11  Analyses of pandemic preparedness policies rev... -1.965877     6  \n",
      "6   BACKGROUND: During the 2009/A/H1N1 pandemic, t... -2.158000     7  \n",
      "2   OBJECTIVE: To evaluate the effect of the COVID... -2.159045     8  \n",
      "3   OBJECTIVE: To evaluate the effect of the COVID... -2.159045     9  \n",
      "5   BACKGROUND: Community-wide preparedness for pa... -2.333743    10  \n",
      "1   OBJECTIVE To evaluate the effect of the COVID-... -2.491803    11  \n",
      "7   In this paper, we explore the possible policy ... -5.268444    12  \n",
      "13  BACKGROUND Humans have faced 3 major influenza... -5.521738    13  \n",
      "8   In this paper, we explore the possible policy ... -6.342962    14  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:28:56] \"GET /query?searchString=pandemic HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Apr/2023 18:29:37] \"OPTIONS /feedback HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Apr/2023 18:29:39] \"POST /feedback HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sum_of_rel_doc_vectors is zero\n",
      "Length of sum_of_irrel_doc_vectors is zero\n",
      "Length of sum_of_rel_key_vectors is zero\n",
      "Length of sum_of_irrel_key_vectors is zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:29:51] \"GET /plot HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry cough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5:  50%|█████     | 2/4 [00:18<00:17,  8.92s/batches]Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "monoT5: 100%|██████████| 4/4 [00:34<00:00,  8.57s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid   docid     docno      query  \\\n",
      "9    1   16448  xl1z3xp3  dry cough   \n",
      "1    1  103389  25k0nz3k  dry cough   \n",
      "7    1  147416  9c1oqbn0  dry cough   \n",
      "0    1   41106  purbi7cc  dry cough   \n",
      "2    1  145248  ub0gyqst  dry cough   \n",
      "4    1   70792  nvtlj7ln  dry cough   \n",
      "5    1  140949  nvtlj7ln  dry cough   \n",
      "3    1  100461  f4slzyvl  dry cough   \n",
      "10   1  141151  azevdbet  dry cough   \n",
      "6    1  163215  9pdakwmb  dry cough   \n",
      "11   1   57158  edfey1q2  dry cough   \n",
      "12   1   70213  ygf7z5mt  dry cough   \n",
      "8    1  156596  lqniqui0  dry cough   \n",
      "13   1   63975  q0evfz24  dry cough   \n",
      "14   1  128140  e64qw1tb  dry cough   \n",
      "\n",
      "                                             abstract      score  rank  \n",
      "9   A 62-year-old man, Mr. M., comes to your offic...  -0.814797     0  \n",
      "1   HISTORY: A 56-year-old, previously healthy mal...  -3.071942     1  \n",
      "7   BACKGROUND: Acute respiratory illness, a leadi...  -3.292914     2  \n",
      "0   HISTORY A 56-year-old, previously healthy male...  -3.329136     3  \n",
      "2   BACKGROUND: Limitations on testing availabilit...  -4.200008     4  \n",
      "4   We describe the case of a young female patient...  -5.273384     5  \n",
      "5   We describe the case of a young female patient...  -5.273384     6  \n",
      "3   BACKGROUND: Symptom criteria for COVID-19 test...  -5.537506     7  \n",
      "10  The transmission dynamics of highly contagious...  -5.955557     8  \n",
      "6   We are living in times where a viral disease h...  -6.145068     9  \n",
      "11  BACKGROUND We tested the hypothesis that warm-...  -6.583505    10  \n",
      "12  Objective: To observe the clinical effect of S...  -8.982994    11  \n",
      "8   Abstract The cough reflex is an attack of powe... -10.193797    12  \n",
      "13  OBJECTIVE To investigate traditional Chinese m... -12.592371    13  \n",
      "14  OBJECTIVE: To investigate traditional Chinese ... -12.664350    14  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:31:04] \"GET /query?searchString=dry%20cough HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry cough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|██████████| 4/4 [00:38<00:00,  9.65s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid   docid     docno      query  \\\n",
      "9    1   16448  xl1z3xp3  dry cough   \n",
      "1    1  103389  25k0nz3k  dry cough   \n",
      "7    1  147416  9c1oqbn0  dry cough   \n",
      "0    1   41106  purbi7cc  dry cough   \n",
      "2    1  145248  ub0gyqst  dry cough   \n",
      "4    1   70792  nvtlj7ln  dry cough   \n",
      "5    1  140949  nvtlj7ln  dry cough   \n",
      "3    1  100461  f4slzyvl  dry cough   \n",
      "10   1  141151  azevdbet  dry cough   \n",
      "6    1  163215  9pdakwmb  dry cough   \n",
      "11   1   57158  edfey1q2  dry cough   \n",
      "12   1   70213  ygf7z5mt  dry cough   \n",
      "8    1  156596  lqniqui0  dry cough   \n",
      "13   1   63975  q0evfz24  dry cough   \n",
      "14   1  128140  e64qw1tb  dry cough   \n",
      "\n",
      "                                             abstract      score  rank  \n",
      "9   A 62-year-old man, Mr. M., comes to your offic...  -0.814797     0  \n",
      "1   HISTORY: A 56-year-old, previously healthy mal...  -3.071942     1  \n",
      "7   BACKGROUND: Acute respiratory illness, a leadi...  -3.292914     2  \n",
      "0   HISTORY A 56-year-old, previously healthy male...  -3.329136     3  \n",
      "2   BACKGROUND: Limitations on testing availabilit...  -4.200008     4  \n",
      "4   We describe the case of a young female patient...  -5.273384     5  \n",
      "5   We describe the case of a young female patient...  -5.273384     6  \n",
      "3   BACKGROUND: Symptom criteria for COVID-19 test...  -5.537506     7  \n",
      "10  The transmission dynamics of highly contagious...  -5.955557     8  \n",
      "6   We are living in times where a viral disease h...  -6.145068     9  \n",
      "11  BACKGROUND We tested the hypothesis that warm-...  -6.583505    10  \n",
      "12  Objective: To observe the clinical effect of S...  -8.982994    11  \n",
      "8   Abstract The cough reflex is an attack of powe... -10.193797    12  \n",
      "13  OBJECTIVE To investigate traditional Chinese m... -12.592371    13  \n",
      "14  OBJECTIVE: To investigate traditional Chinese ... -12.664350    14  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:32:51] \"GET /query?searchString=dry%20cough HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'list'>\n",
      "dry cough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|██████████| 4/4 [00:39<00:00,  9.97s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid   docid     docno      query  \\\n",
      "9    1   16448  xl1z3xp3  dry cough   \n",
      "1    1  103389  25k0nz3k  dry cough   \n",
      "7    1  147416  9c1oqbn0  dry cough   \n",
      "0    1   41106  purbi7cc  dry cough   \n",
      "2    1  145248  ub0gyqst  dry cough   \n",
      "4    1   70792  nvtlj7ln  dry cough   \n",
      "5    1  140949  nvtlj7ln  dry cough   \n",
      "3    1  100461  f4slzyvl  dry cough   \n",
      "10   1  141151  azevdbet  dry cough   \n",
      "6    1  163215  9pdakwmb  dry cough   \n",
      "11   1   57158  edfey1q2  dry cough   \n",
      "12   1   70213  ygf7z5mt  dry cough   \n",
      "8    1  156596  lqniqui0  dry cough   \n",
      "13   1   63975  q0evfz24  dry cough   \n",
      "14   1  128140  e64qw1tb  dry cough   \n",
      "\n",
      "                                             abstract      score  rank  \n",
      "9   A 62-year-old man, Mr. M., comes to your offic...  -0.814797     0  \n",
      "1   HISTORY: A 56-year-old, previously healthy mal...  -3.071942     1  \n",
      "7   BACKGROUND: Acute respiratory illness, a leadi...  -3.292914     2  \n",
      "0   HISTORY A 56-year-old, previously healthy male...  -3.329136     3  \n",
      "2   BACKGROUND: Limitations on testing availabilit...  -4.200008     4  \n",
      "4   We describe the case of a young female patient...  -5.273384     5  \n",
      "5   We describe the case of a young female patient...  -5.273384     6  \n",
      "3   BACKGROUND: Symptom criteria for COVID-19 test...  -5.537506     7  \n",
      "10  The transmission dynamics of highly contagious...  -5.955557     8  \n",
      "6   We are living in times where a viral disease h...  -6.145068     9  \n",
      "11  BACKGROUND We tested the hypothesis that warm-...  -6.583505    10  \n",
      "12  Objective: To observe the clinical effect of S...  -8.982994    11  \n",
      "8   Abstract The cough reflex is an attack of powe... -10.193797    12  \n",
      "13  OBJECTIVE To investigate traditional Chinese m... -12.592371    13  \n",
      "14  OBJECTIVE: To investigate traditional Chinese ... -12.664350    14  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:35:20] \"GET /query?searchString=dry%20cough HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:36:27] \"OPTIONS /feedback HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Apr/2023 18:36:32] \"POST /feedback HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sum_of_rel_doc_vectors is zero\n",
      "Length of sum_of_irrel_doc_vectors is zero\n",
      "Length of sum_of_rel_key_vectors is zero\n",
      "Length of sum_of_irrel_key_vectors is zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:36:34] \"GET /plot HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandemic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monoT5: 100%|██████████| 4/4 [00:42<00:00, 10.52s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qid   docid     docno     query  \\\n",
      "4    1   52415  8n0mwad5  pandemic   \n",
      "10   1  161087  9s7y8guc  pandemic   \n",
      "14   1  134190  p94jups3  pandemic   \n",
      "0    1     513  dzzlpz4o  pandemic   \n",
      "12   1   61346  v932f5ni  pandemic   \n",
      "9    1  177207  5nkrrqqw  pandemic   \n",
      "11   1   40347  4upkfpuz  pandemic   \n",
      "6    1  149026  onyoh6lo  pandemic   \n",
      "2    1  111087  1c2rnwjb  pandemic   \n",
      "3    1  111088  e171iero  pandemic   \n",
      "5    1  192498  08gqn86z  pandemic   \n",
      "1    1   48139  313bi7q4  pandemic   \n",
      "7    1   69605  wp9dpiqs  pandemic   \n",
      "13   1   88814  n80jy8zl  pandemic   \n",
      "8    1  179379  wp9dpiqs  pandemic   \n",
      "\n",
      "                                             abstract     score  rank  \n",
      "4   The 1918 to 1919 H1N1 influenza pandemic is am... -0.711614     0  \n",
      "10  Summary It is commonly believed that the clini... -1.087005     1  \n",
      "14  Few viruses have shaped the course of human hi... -1.275363     2  \n",
      "0   BACKGROUND: More than a year after an influenz... -1.449998     3  \n",
      "12  Influenza pandemics have occurred throughout h... -1.786306     4  \n",
      "9   Summary The 2009 H1N1 influenza pandemic was a... -1.940994     5  \n",
      "11  Analyses of pandemic preparedness policies rev... -1.965877     6  \n",
      "6   BACKGROUND: During the 2009/A/H1N1 pandemic, t... -2.158000     7  \n",
      "2   OBJECTIVE: To evaluate the effect of the COVID... -2.159045     8  \n",
      "3   OBJECTIVE: To evaluate the effect of the COVID... -2.159045     9  \n",
      "5   BACKGROUND: Community-wide preparedness for pa... -2.333743    10  \n",
      "1   OBJECTIVE To evaluate the effect of the COVID-... -2.491803    11  \n",
      "7   In this paper, we explore the possible policy ... -5.268444    12  \n",
      "13  BACKGROUND Humans have faced 3 major influenza... -5.521738    13  \n",
      "8   In this paper, we explore the possible policy ... -6.342962    14  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n",
      "Type of record:  <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:37:56] \"GET /query?searchString=pandemic HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Apr/2023 18:38:08] \"OPTIONS /feedback HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Apr/2023 18:38:11] \"POST /feedback HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sum_of_rel_doc_vectors is zero\n",
      "Length of sum_of_irrel_doc_vectors is zero\n",
      "Length of sum_of_rel_key_vectors is zero\n",
      "Length of sum_of_irrel_key_vectors is zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Apr/2023 18:38:14] \"GET /plot HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# app.config[\"DEBUG\"] = True\n",
    "CORS(app)\n",
    "from flask import send_file\n",
    "@app.route('/query', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('searchString')\n",
    "    print(query)\n",
    "    searchResults = search_query(query)\n",
    "    # print(searchResults.head())\n",
    "    searchResultswithkeys = key_phrasification(searchResults)\n",
    "    # print(searchResultswithkeys)\n",
    "\n",
    "    for index, row in searchResultswithkeys.iterrows():\n",
    "        # print('Type of record: ', type(row['KeyList']))\n",
    "        # keylist = ast.literal_eval(row['KeyList'])\n",
    "        keylist = row['KeyList']\n",
    "        searchResultswithkeys.loc[index, 'context_key_1'] = extract_context(row['abstract'] + row['title'], keylist[0])\n",
    "        searchResultswithkeys.loc[index, 'context_key_2'] = extract_context(row['abstract'] + row['title'], keylist[1])\n",
    "        searchResultswithkeys.loc[index, 'context_key_3'] = extract_context(row['abstract'] + row['title'], keylist[2])\n",
    "        searchResultswithkeys.loc[index, 'context_key_4'] = extract_context(row['abstract'] + row['title'], keylist[3])\n",
    "        searchResultswithkeys.loc[index, 'context_key_5'] = extract_context(row['abstract'] + row['title'], keylist[4])\n",
    "    # searchResultswithkeys = searchResultswithkeys.drop(columns=['Unnamed: 0'])\n",
    "    global query_df\n",
    "    query_dict = {'query': [query], 'query_vec' : [np.nan], 'new_query_vec' : [np.nan] }\n",
    "    query_df = pd.DataFrame(data=query_dict, index=[0])\n",
    "    # print(searchResultswithkeys)\n",
    "    return searchResultswithkeys.to_json(orient='records')\n",
    "\n",
    "# Feedback\n",
    "@app.route('/feedback', methods=['POST'])\n",
    "def fetchFeedback():\n",
    "    global reranked_df\n",
    "    feedbackJson = request.json['updates']\n",
    "    relevanceList = []\n",
    "    if len(feedbackJson):\n",
    "        for doc in feedbackJson:\n",
    "            for relevance in doc['value']:\n",
    "                relevanceList.append(relevance)\n",
    "    feedback_df = pd.DataFrame(relevanceList)\n",
    "    feedback_df.drop(columns=['bntStyle'], inplace=True)\n",
    "    reranked_df = get_reranking(feedback_df)\n",
    "    return reranked_df.to_json(orient='records')\n",
    "\n",
    "@app.route('/plot')\n",
    "def plot():\n",
    "    # Generate the plot\n",
    "\n",
    "\n",
    "    fig = generate_plot()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    output = io.BytesIO()\n",
    "    canvas.print_png(output)\n",
    "    response = Response(output.getvalue(), mimetype='image/png')\n",
    "\n",
    "    return response\n",
    "    # buffer.seek(0)\n",
    "\n",
    "    # image_png = buffer.getvalue()\n",
    "    # buffer.close()\n",
    "    # graphic = base64.b64encode(image_png).decode('utf-8')\n",
    "    # graphic = graphic.decode('utf-8')\n",
    "    # fig.savefig('plot.png')\n",
    "\n",
    "    # return send_file('plot.png', mimetype='image/png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    app.run(host='0.0.0.0',port=5132)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Feedback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_reranking(feedback_df : pd.DataFrame, model):\n",
    "    global  query_df\n",
    "    feedback_df[\"summary_vec\"] = get_vectors(feedback_df['summary'], model)\n",
    "    feedback_df[\"keyList_vec\"] = get_vectors(feedback_df['KeyList'], model)\n",
    "    relevant_df = feedback_df.loc[feedback_df['relevant'] == True]\n",
    "    irrelevant_df = feedback_df.loc[feedback_df['relevant'] == False]\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], relevant_df[\"summary_vec\"].values, irrelevant_df[\"summary_vec\"].values, relevant_df[\"keyList_vec\"].values, irrelevant_df[\"keyList_vec\"].values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, feedback_df, \"keyList_vec\")\n",
    "    return sorted_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cal_RM():\n",
    "    data = pd.read_csv('/Users/GovindShukla/Desktop/Information-Retrieval-Project/search_result.csv')\n",
    "    print(data['summary'])\n",
    "    model = define_d2v_model(data['abstract'])\n",
    "    q = {'query': ['Covid19'], 'query_vec' : [np.nan], 'new_query_vec' : [np.nan] }\n",
    "    query_df = pd.DataFrame(data=q, index=[0])\n",
    "    data[\"summary_vec\"] = get_vectors(data['summary'], model)\n",
    "    data[\"keyList_vec\"] = get_vectors(data['KeyList'], model)\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], data[\"summary_vec\"].head(3).values, data[\"summary_vec\"].head(7).values,\n",
    "                      data[\"keyList_vec\"].head(3).values, data[\"keyList_vec\"].head(7).values, 1.0, 0.5,1.0, 0.5)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, data, \"keyList_vec\")\n",
    "    print(sorted_dataset['summary'])\n",
    "cal_RM()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rel_docs = []\n",
    "irrel_docs = []\n",
    "for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['summary_vec'].values):\n",
    "    rel_docs.append(data)\n",
    "for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == False]['summary_vec'].values):\n",
    "    irrel_docs.append(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reranked_df.loc[reranked_df['relevant'] == False]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reranked_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_plot():\n",
    "    rel_docs = []\n",
    "    irrel_docs = []\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['summary_vec'].values):\n",
    "        rel_docs.append(data)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == False]['summary_vec'].values):\n",
    "        irrel_docs.append(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_rel = pca.fit_transform(rel_docs)\n",
    "    pca_irrel = pca.fit_transform(irrel_docs)\n",
    "    print(query_df)\n",
    "    query =np.concatenate(([np.array(query_df.iloc[0]['query_vec'])], [np.array(query_df.iloc[0]['new_query_vec'])]), axis=0)\n",
    "    pca_query = pca.fit_transform(query)\n",
    "\n",
    "    plt.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"rel_doc\")\n",
    "    plt.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrel_doc\")\n",
    "    plt.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    plt.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    plt.legend()\n",
    "\n",
    "    return plt\n",
    "\n",
    "generate_plot()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}
