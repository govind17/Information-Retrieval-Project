{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/govind17/Information-Retrieval-Project/blob/main/LDA_feature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (2.2.3)\r\n",
      "Collecting Flask\r\n",
      "  Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 96 kB 3.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: Jinja2>=3.1.2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Flask) (3.1.2)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Flask) (2.1.2)\r\n",
      "Collecting Werkzeug>=2.3.3\r\n",
      "  Downloading Werkzeug-2.3.4-py3-none-any.whl (242 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 242 kB 4.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting blinker>=1.6.2\r\n",
      "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\r\n",
      "Collecting click>=8.1.3\r\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask) (2.1.1)\r\n",
      "Installing collected packages: Werkzeug, click, blinker, Flask\r\n",
      "  Attempting uninstall: Werkzeug\r\n",
      "    Found existing installation: Werkzeug 2.2.2\r\n",
      "    Uninstalling Werkzeug-2.2.2:\r\n",
      "      Successfully uninstalled Werkzeug-2.2.2\r\n",
      "  Attempting uninstall: click\r\n",
      "    Found existing installation: click 8.0.4\r\n",
      "    Uninstalling click-8.0.4:\r\n",
      "      Successfully uninstalled click-8.0.4\r\n",
      "  Attempting uninstall: blinker\r\n",
      "    Found existing installation: blinker 1.4\r\n",
      "    Uninstalling blinker-1.4:\r\n",
      "      Successfully uninstalled blinker-1.4\r\n",
      "  Attempting uninstall: Flask\r\n",
      "    Found existing installation: Flask 2.2.3\r\n",
      "    Uninstalling Flask-2.2.3:\r\n",
      "      Successfully uninstalled Flask-2.2.3\r\n",
      "Successfully installed Flask-2.3.2 Werkzeug-2.3.4 blinker-1.6.2 click-8.1.3\r\n",
      "Collecting en-core-web-sm==3.4.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 12.8 MB 5.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.22.3)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U Flask\n",
    "# !pip install git+https://github.com/boudinfl/pke.git\n",
    "# !pip install matplotlib\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask-script in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (2.0.6)\r\n",
      "Requirement already satisfied: Flask in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from flask-script) (2.2.3)\r\n",
      "Requirement already satisfied: click>=8.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Flask->flask-script) (8.0.4)\r\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Flask->flask-script) (2.2.2)\r\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Flask->flask-script) (2.1.2)\r\n",
      "Requirement already satisfied: Jinja2>=3.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Flask->flask-script) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/GovindShukla/miniforge3/envs/GSCS/lib/python3.10/site-packages (from Jinja2>=3.0->Flask->flask-script) (2.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install flask-script\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/GovindShukla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/GovindShukla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One time installation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import io\n",
    "# import flask_script\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import flask\n",
    "from flask import request\n",
    "from flask_cors import CORS\n",
    "from flask import Flask, Response\n",
    "from keyPhrasification import key_phrasification\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pyterrier as pt\n",
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import ast\n",
    "from sklearn.manifold import TSNE\n",
    "from keyPhrasification import key_phrasification\n",
    "# from flask_script import Manager\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.1 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Indexing the cord-19 dataset\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = 'C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/terrier_cord19'\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # create the index, using the IterDictIndexer indexer\n",
    "    indexer = pt.index.IterDictIndexer(pt_index_path)\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    index_ref = indexer.index(cord19.get_corpus_iter(),\n",
    "                              fields=('abstract',),\n",
    "                              meta=('docno',))\n",
    "else:\n",
    "    # if you already have the index, use it.\n",
    "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "    index = pt.IndexFactory.of(index_ref)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "def pre_processing_data(data):\n",
    "    ps = PorterStemmer()\n",
    "    tagged_dataset = [TaggedDocument(words=[ps.stem(w) for w in nltk.word_tokenize(str(_d)) if word_tokenize(str(_d).lower()) not in stopwords.words('english')], tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    #tagged_dataset = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    return tagged_dataset\n",
    "\n",
    "def model_doc2vec(model, tagged_data, num_epochs):\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,total_examples=len(tagged_data), epochs=num_epochs)\n",
    "    return model\n",
    "\n",
    "def vector_for_learning(model, input_docs):\n",
    "    sents = input_docs\n",
    "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, feature_vectors\n",
    "\n",
    "def define_d2v_model(dataset):\n",
    "    preprocessed_tagged_dataset = pre_processing_data(dataset)\n",
    "    model_d2v = Doc2Vec(dm=1, vector_size=100, window = 10, negative=5, hs=0, min_count=2, sample = 0, alpha=0.025, min_alpha=0.001, dm_mean = 0, dbow_words=1)\n",
    "    model = model_doc2vec(model_d2v, preprocessed_tagged_dataset, 100)\n",
    "    return model\n",
    "\n",
    "def get_vectors(input_data, model):\n",
    "    preprocessed_tagged_input_data= pre_processing_data(input_data)\n",
    "    id, vectors = np.array(vector_for_learning(model, preprocessed_tagged_input_data), dtype=object)\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Rocchio algorithm\n",
    "def rocchio_algorithm(query_doc_vector, docs_relevant_vectors,\n",
    "                      docs_irrelevant_vectors, key_relevant_vectors,\n",
    "                      key_irrelevant_vectors,\n",
    "                      alpha, beta, gamma, delta):\n",
    "\n",
    "    # sum_of_rel_doc_vectors = docs_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_doc_vectors', sum_of_rel_doc_vectors)\n",
    "    sum_of_rel_doc_vectors=[]\n",
    "    sum_of_irrel_doc_vectors=[]\n",
    "    sum_of_rel_key_vectors=[]\n",
    "    sum_of_irrel_key_vectors=[]\n",
    "    for each_rel_doc_vector in docs_relevant_vectors:\n",
    "        if len(sum_of_rel_doc_vectors) == 0:\n",
    "            # print('Length of sum_of_rel_doc_vectors is zero')\n",
    "            sum_of_rel_doc_vectors = each_rel_doc_vector\n",
    "        else:\n",
    "            # print(each_rel_doc_vector)\n",
    "            sum_of_rel_doc_vectors = list(map(operator.add, sum_of_rel_doc_vectors, each_rel_doc_vector))\n",
    "            # print('SUM :', sum_of_rel_doc_vectors)\n",
    "    for each_irrel_doc_vector in docs_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_doc_vectors) == 0:\n",
    "            sum_of_irrel_doc_vectors = each_irrel_doc_vector\n",
    "        else:\n",
    "            sum_of_irrel_doc_vectors = list(map(operator.add, sum_of_irrel_doc_vectors, each_irrel_doc_vector))\n",
    "    # sum_of_irrel_doc_vectors = docs_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_doc_vectors', sum_of_irrel_doc_vectors)\n",
    "    for each_rel_key_vector in key_relevant_vectors:\n",
    "        if len(sum_of_rel_key_vectors) == 0:\n",
    "            sum_of_rel_key_vectors = each_rel_key_vector\n",
    "        else:\n",
    "            sum_of_rel_key_vectors = list(map(operator.add, sum_of_rel_key_vectors, each_rel_key_vector))\n",
    "    # sum_of_rel_key_vectors = key_relevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_rel_key_vectors', sum_of_rel_key_vectors)\n",
    "    for each_irrel_key_vector in key_irrelevant_vectors:\n",
    "        if len(sum_of_irrel_key_vectors) == 0:\n",
    "            sum_of_irrel_key_vectors = each_irrel_key_vector\n",
    "        else:\n",
    "            sum_of_irrel_key_vectors = list(map(operator.add, sum_of_irrel_key_vectors, each_irrel_key_vector))\n",
    "    # sum_of_irrel_key_vectors = key_irrelevant_vectors.sum(axis=0)\n",
    "    # print('sum_of_irrel_key_vectors', (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors))\n",
    "\n",
    "    new_doc_vector_query = np.sum([np.array(query_doc_vector)\n",
    "    , (alpha/len(docs_relevant_vectors)) * np.array(sum_of_rel_doc_vectors)\n",
    "    , (beta/len(docs_irrelevant_vectors)) * np.array(sum_of_irrel_doc_vectors)\n",
    "    , (gamma/len(docs_relevant_vectors)) * np.array(sum_of_rel_key_vectors)\n",
    "    , (delta/len(docs_relevant_vectors)) * np.array(sum_of_irrel_key_vectors)], axis=0)\n",
    "    df = pd.DataFrame({\"a\": [new_doc_vector_query]})\n",
    "    return df.values\n",
    "\n",
    "# Compute cosine scores\n",
    "def compute_cosine_sim(new_query_vector, all_data, name):\n",
    "    consine_similarities = []\n",
    "    for index, data in all_data.iterrows():\n",
    "        cosine_sim = cosine_similarity([np.array(new_query_vector[0])], [np.array(all_data[name][index])])\n",
    "        consine_similarities.append(cosine_sim[0][0])\n",
    "    return consine_similarities\n",
    "\n",
    "# Rank data\n",
    "def rank_data(new_query_vector, dataset, name, a, b):\n",
    "    ps = PorterStemmer()\n",
    "    cosine_sim_values = compute_cosine_sim(new_query_vector, dataset, name)\n",
    "    dataset['Cosine_Similarity_' + name] = cosine_sim_values\n",
    "    # dataset.drop(dataset[dataset['bntStyle'] == False].index, inplace=True)\n",
    "    dataset['contexts'] = dataset['contexts'].apply(ast.literal_eval)\n",
    "    rel_keyList_df = dataset['KeyList'][dataset['relevant'] == True]\n",
    "    relevant_KeyList = []\n",
    "    for lst in rel_keyList_df:\n",
    "        # lst = list(map(lambda word: ps.stem(word), lst))\n",
    "        relevant_KeyList.extend(lst)\n",
    "\n",
    "    # dataset['Key_word_match_count'] = 0\n",
    "    dataset['Keyword_match_dict'] = [{} for _ in range(len(dataset))]\n",
    "    dataset['Key_word_match_count'] = 0\n",
    "    dataset['matched_contexts'] = [[] for _ in range(len(dataset))]\n",
    "    for i, row in dataset.iterrows():\n",
    "        matched_context = []\n",
    "        key_match_dict = {phrase: 0 for phrase in relevant_KeyList}\n",
    "        contexts = row['contexts']\n",
    "        for context in contexts:\n",
    "            # words = [ps.stem(w.lower()) for w in nltk.word_tokenize(context)]\n",
    "            for phrase in relevant_KeyList:\n",
    "                # word_list = ps.stem(word).split()\n",
    "                # set1 = set(word_list)\n",
    "                # set2 = set(words)\n",
    "                # print(\"Set1\",set1)\n",
    "                # print(\"Set2\",set2)\n",
    "                # if set1.issubset(set2):\n",
    "                if phrase.lower() in context.lower():\n",
    "                    key_match_dict[phrase] += 1\n",
    "                    dataset.at[i, 'Key_word_match_count']+=1\n",
    "                    matched_context.append(context)\n",
    "        dataset.at[i, 'Keyword_match_dict'] = key_match_dict\n",
    "        if matched_context is not []:\n",
    "            dataset.at[i,'matched_contexts'] = list(set(matched_context))\n",
    "        else:\n",
    "            dataset.at[i,'matched_contexts'] = matched_context\n",
    "    # columns_to_search = ['context_key_1', 'context_key_2', 'context_key_3', 'context_key_4', 'context_key_5']\n",
    "    # for i, row in dataset.iterrows():\n",
    "    #     for col in dataset[columns_to_search].columns:\n",
    "    #         if row[col] is None:\n",
    "    #             continue\n",
    "    #         words = [ps.stem(w.lower()) for w in nltk.word_tokenize(row[col])]\n",
    "    #         for word in relevant_KeyList:\n",
    "    #             word_list = word.split()\n",
    "    #             set1 = set(word_list)\n",
    "    #             set2 = set(words)\n",
    "    #             if set1.intersection(set2):\n",
    "    #                 dataset.at[i, 'Key_word_match_count'] += 1\n",
    "    # dataset['feedback_score'] = dataset['Key_word_match_count']/5 + dataset['Cosine_Similarity_' + name]\n",
    "    dataset['feedback_score'] = a * dataset['Cosine_Similarity_' + name] / np.linalg.norm(dataset['Cosine_Similarity_' + name]) + b * dataset['Key_word_match_count'] / np.linalg.norm(dataset['Key_word_match_count'])\n",
    "    # sorted_dataset = dataset.sort_values(by=['feedback_score'], ascending=False)\n",
    "    sorted_dataset = dataset.sort_values(by=['feedback_score'], ascending=False)\n",
    "    return sorted_dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pritha\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "monoT5 = MonoT5ReRanker(text_field='abstract')\n",
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "cord19_df = pd.read_csv('C:/Users/Pritha/Desktop/SUBJECTS/PROJECT/Relevance feedback with XAI/Backend Code/Information-Retrieval-Project/cord19_context.csv')\n",
    "# model = define_d2v_model(cord19_df['abstract'])\n",
    "model = joblib.load('gensim_model_2.pkl')\n",
    "query_df = pd.DataFrame\n",
    "reranked_df = pd.DataFrame\n",
    "\n",
    "def search_query(query, name):\n",
    "  index_ref2 = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "  index2 = pt.IndexFactory.of(index_ref2)\n",
    "  # print(query)\n",
    "  if not pt.started():\n",
    "      pt.init()\n",
    "  br = pt.BatchRetrieve(index2) % 50\n",
    "  pipeline = (br >> pt.text.get_text(dataset, 'abstract') >> monoT5)\n",
    "  search_result = pipeline.search(query)\n",
    "  # print(search_result)\n",
    "  search_result = search_result.drop_duplicates(subset=['docno'])\n",
    "  # search_result = search_result.drop_duplicates(subset=['summary'])\n",
    "\n",
    "  filtered_docs = pd.merge(cord19_df, search_result, on = \"docno\", how = \"inner\")\n",
    "  filtered_docs.rename(columns={'abstract_x':'abstract'}, inplace=True)\n",
    "  filtered_docs.drop(columns='abstract_y', axis=1, inplace=True)\n",
    "  filtered_docs['KeyList'] = filtered_docs['KeyList'].apply(ast.literal_eval)\n",
    "  # filtered_docs.to_csv('search_result_covid.csv')\n",
    "    # filtered_docs.to_csv('search_result_cough.csv')\n",
    "    # filtered_docs.to_csv('search_result_pandemic.csv')\n",
    "  filtered_docs.to_csv(name)\n",
    "    # filtered_docs.to_csv('search_result_pandemicDeath.csv')\n",
    "    # filtered_docs.to_csv('search_result_hosBedAvb.csv')\n",
    "  # filtered_docs = filtered_docs.drop_duplicates(subset=['docno', 'summary'])\n",
    "  # return filtered_docs\n",
    "  return filtered_docs\n",
    "\n",
    "\n",
    "# search_query('covid')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_reranking(feedback_df : pd.DataFrame, a, b):\n",
    "    global  query_df\n",
    "    feedback_df[\"doc_vec\"] = get_vectors(feedback_df['abstract'], model)\n",
    "    feedback_df[\"keyList_vec\"] = get_vectors(feedback_df['KeyList'], model)\n",
    "    relevant_df = feedback_df.head(10).loc[feedback_df['relevant'] == True]\n",
    "    irrelevant_df = feedback_df.head(10).loc[feedback_df['relevant'] == False]\n",
    "    query_df[\"query_vec\"]  = get_vectors(query_df['query'].values, model)\n",
    "    query_df['new_query_vec'] = rocchio_algorithm(query_df[\"query_vec\"][0], relevant_df[\"doc_vec\"].values, irrelevant_df[\"doc_vec\"].values, relevant_df[\"keyList_vec\"].values, irrelevant_df[\"keyList_vec\"].values, 0.8, 0.3,0.8, 0.3)\n",
    "    sorted_dataset = rank_data(query_df['new_query_vec'].values, feedback_df, \"keyList_vec\", a, b)\n",
    "    return sorted_dataset\n",
    "\n",
    "def generate_plot():\n",
    "    rel_vecs = []\n",
    "    irrel_vecs = []\n",
    "    new_reranked_rel_vecs = []\n",
    "    # plt.canvas.flush_events()\n",
    "    print(\"Reranked DF: \", reranked_df)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['keyList_vec'].values):\n",
    "        rel_vecs.append(data)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['bntStyle'] == False]['keyList_vec'].values):\n",
    "        irrel_vecs.append(data)\n",
    "    for i, data in enumerate(reranked_df.head(10).loc[reranked_df['relevant'] == False]['keyList_vec'].values):\n",
    "        new_reranked_rel_vecs.append(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_rel = pca.fit_transform(rel_vecs)\n",
    "    print(\"Irrelevant vecs: \",irrel_vecs)\n",
    "    pca_irrel = pca.fit_transform(irrel_vecs)\n",
    "    pca_new_rel = pca.fit_transform(new_reranked_rel_vecs)\n",
    "    query =np.concatenate(([np.array(query_df.iloc[0]['query_vec'])], [np.array(query_df.iloc[0]['new_query_vec'])]), axis=0)\n",
    "    pca_query = pca.fit_transform(query)\n",
    "    fig = Figure()\n",
    "    axis = fig.add_subplot(1, 1, 1)\n",
    "    axis.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"relevant\")\n",
    "    axis.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrelevant\")\n",
    "    axis.scatter(pca_new_rel[:,0], pca_new_rel[:,1], color='yellow', marker=\"d\", label=\"new relevant\")\n",
    "    axis.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    axis.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    axis.set_xlabel(\"PCA 1\")\n",
    "    axis.set_ylabel(\"PCA 2\")\n",
    "    axis.set_title(\"Visualization of vectors\")\n",
    "    axis.axvline(x=0, color='gray', linestyle='--')\n",
    "    axis.legend()\n",
    "\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def generate_plot2():\n",
    "    rel_vecs = []\n",
    "    irrel_vecs = []\n",
    "    new_reranked_rel_vecs = []\n",
    "    print(\"Reranked DF: \", reranked_df)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['relevant'] == True]['keyList_vec'].values):\n",
    "        rel_vecs.append(data)\n",
    "    for i, data in enumerate(reranked_df.loc[reranked_df['bntStyle'] == False]['keyList_vec'].values):\n",
    "        irrel_vecs.append(data)\n",
    "    for i, data in enumerate(reranked_df.head(10).loc[reranked_df['relevant'] == False]['keyList_vec'].values):\n",
    "        new_reranked_rel_vecs.append(data)\n",
    "    all_vecs = np.concatenate((rel_vecs, irrel_vecs, new_reranked_rel_vecs), axis=0)\n",
    "    pca = TSNE(n_components=2, random_state = 0)\n",
    "    pca_all = pca.fit_transform(all_vecs)\n",
    "    pca_rel = pca_all[:len(rel_vecs)]\n",
    "    pca_irrel = pca_all[len(rel_vecs):len(rel_vecs)+len(irrel_vecs)]\n",
    "    pca_new_rel = pca_all[len(rel_vecs)+len(irrel_vecs):]\n",
    "    query =np.concatenate(([np.array(query_df.iloc[0]['query_vec'])], [np.array(query_df.iloc[0]['new_query_vec'])]), axis=0)\n",
    "    pca_query = pca.fit_transform(query)\n",
    "    fig = Figure()\n",
    "    axis = fig.add_subplot(1, 1, 1)\n",
    "    axis.scatter(pca_rel[:,0], pca_rel[:,1], marker=\"8\", label=\"relevant\")\n",
    "    axis.scatter(pca_irrel[:,0], pca_irrel[:,1], marker=\"*\", label=\"irrelevant\")\n",
    "    axis.scatter(pca_new_rel[:,0], pca_new_rel[:,1], color='yellow', marker=\"d\", label=\"new relevant\")\n",
    "    axis.scatter(pca_query[0][0], pca_query[0][1], marker=\"s\", label=\"old_query\")\n",
    "    axis.scatter(pca_query[1][0], pca_query[1][1], marker=\"s\", label=\"new_query\")\n",
    "    axis.axhline(y=0, linestyle='--', color='gray')\n",
    "    axis.axvline(x=0, linestyle='--', color='gray')\n",
    "    axis.set_xlabel('PCA 1')\n",
    "    axis.set_ylabel('PCA 2')\n",
    "    axis.set_title('Visualization of vectors')\n",
    "    axis.legend()\n",
    "\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# cord19_df = pd.read_csv('/Users/GovindShukla/Downloads/cord19_sum_key.csv')\n",
    "#\n",
    "# # Save the model as a pickle in a file\n",
    "# joblib.dump(model, 'gensim_model_2.pkl')\n",
    "model = joblib.load('/Users/GovindShukla/Desktop/Information-Retrieval-Project/gensim_model_2.pkl')\n",
    "\n",
    "# model = define_d2v_model(cord19_df[~cord19_df['abstract'].isna()]['abstract'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m\u001B[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001B[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5273\n",
      " * Running on http://192.168.0.100:5273\n",
      "\u001B[33mPress CTRL+C to quit\u001B[0m\n",
      "127.0.0.1 - - [29/May/2023 18:04:21] \"GET /query?searchString=p HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/May/2023 18:04:27] \"OPTIONS /feedback HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1     docno                                              title  \\\n",
      "0             0  jb8228vn  Acute care utilization due to hospitalizations...   \n",
      "1             1  tmydzyiq  Improving the Hospital Quality of Care during ...   \n",
      "2             2  x57bhyhr  German hospital capacities for prolonged mecha...   \n",
      "3             3  okkuoefa  Critical care services in Ontario: a survey-ba...   \n",
      "4             4  8ntlmfln  Cancellation of scheduled procedures as a mech...   \n",
      "5             5  5wjbus27  Variation in critical care services across Nor...   \n",
      "6             6  9ffvdgon  Projecting demand for critical care beds durin...   \n",
      "7             7  mjjkdk83  Cost minimisation analysis of thermometry in t...   \n",
      "8             8  d2bh3tm8  Assessing the Hospital Surge Capacity of the K...   \n",
      "9             9  jjtsd4n3  A model to forecast regional demand for COVID-...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  BACKGROUND: Pediatric LRTI hospitalizations ar...   \n",
      "1  BACKGROUND: During each winter the hospital qu...   \n",
      "2  A brief survey among members of the German Neu...   \n",
      "3  PURPOSE In response to the challenges of an ag...   \n",
      "4  BACKGROUND The ability to generate hospital be...   \n",
      "5  OBJECTIVE Critical care represents a large per...   \n",
      "6  BACKGROUND Increasing numbers of coronavirus d...   \n",
      "7  INTRODUCTION Temperature monitoring can be acc...   \n",
      "8  Introduction The COVID-19 pandemic will test t...   \n",
      "9  COVID-19 threatens to overwhelm hospital facil...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  METHODS: LRTI inpatient episodes for patients ...   \n",
      "1  This study explores how QoC-scores could be im...   \n",
      "2  A brief survey among members of the German Neu...   \n",
      "3  Reliable data on available critical care resou...   \n",
      "4  A categorization methodology was devised and a...   \n",
      "5  OBJECTIVE Critical care represents a large per...   \n",
      "6  BACKGROUND Increasing numbers of coronavirus d...   \n",
      "7  INTRODUCTION Temperature monitoring can be acc...   \n",
      "8  Methods We assumed that 2% of the Kenyan popul...   \n",
      "9  We created an interactive, quantitative model ...   \n",
      "\n",
      "                                             KeyList  \\\n",
      "0  [pediatric lrti hospitalizations, years, age, ...   \n",
      "1  [extra hospital beds, rotavirus vaccination co...   \n",
      "2  [beds, weaning, hospital capacities, simultane...   \n",
      "3  [critical care services, beds, number, unit, a...   \n",
      "4  [hospital beds, elective procedures, impact, i...   \n",
      "5  [intensive care units, countries, united state...   \n",
      "6  [self-isolation, bed requirements, peak, criti...   \n",
      "7  [device covidien genius, tat, years, savings, ...   \n",
      "8  [hospitalization, surge capacity, covid-19 pan...   \n",
      "9  [quantitative model, covid-19, intensive care ...   \n",
      "\n",
      "                                            contexts  qid  docid  \\\n",
      "0  ['burden of pediatric LRTIs on hospit', 'were ...    1   1047   \n",
      "1  ['Bed occupancy rates often surpass the', 'goo...    1   3062   \n",
      "2  ['the hospital capacities ( “ beds ” ) for pro...    1  26062   \n",
      "3  ['the provision of critical care servic', 'mai...    1  32080   \n",
      "4  ['BACKGROUND The ability to generate hospital ...    1  51359   \n",
      "5  ['care hospital or intensive care unit beds ac...    1  54213   \n",
      "6  ['evaluated the extent to which self-isolation...    1  57256   \n",
      "7  ['of measurements per device for TAT to be cos...    1  61060   \n",
      "8  ['of people that will need hospitalization and...    1  69533   \n",
      "9  ['quantitative model that forecasts demand for...    1  71834   \n",
      "\n",
      "          query      score  rank bntStyle relevanceToggleText  relevant  \n",
      "0  hospital bed -10.387531    34     True            Relevant      True  \n",
      "1  hospital bed  -9.087610    21     True            Relevant      True  \n",
      "2  hospital bed  -8.486090    14     True            Relevant      True  \n",
      "3  hospital bed  -7.855976     9    False          Irrelevant     False  \n",
      "4  hospital bed  -3.530451     0    False          Irrelevant     False  \n",
      "5  hospital bed  -7.597765     7    False          Irrelevant     False  \n",
      "6  hospital bed  -9.612203    27    False          Irrelevant     False  \n",
      "7  hospital bed -11.255247    41    False          Irrelevant     False  \n",
      "8  hospital bed  -8.602598    16    False          Irrelevant     False  \n",
      "9  hospital bed -11.408668    45    False          Irrelevant     False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/May/2023 18:04:55] \"POST /feedback HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = flask.Flask(__name__)\n",
    "\n",
    "CORS(app)\n",
    "@app.route('/query', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('searchString')\n",
    "    print(query)\n",
    "    # start of permanent code\n",
    "    # searchResults = search_query(query, 'search_result_hosBedAvb.csv')\n",
    "    # start of permanent code\n",
    "\n",
    "    # start of temporary code\n",
    "    # searchResults = pd.read_csv('search_result_cough.csv')\n",
    "    # searchResults = pd.read_csv('search_result_covid.csv')\n",
    "    # searchResults = pd.read_csv('search_result_pandemic.csv')\n",
    "    searchResults = pd.read_csv('search_result_hosBedAvb.csv')\n",
    "    # searchResults = pd.read_csv('search_result_dryCough.csv')\n",
    "    # searchResults = pd.read_csv('search_result_hosBedAvb.csv')\n",
    "    searchResults.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    searchResults['KeyList'] = searchResults['KeyList'].apply(ast.literal_eval)\n",
    "    # end of temporary code\n",
    "\n",
    "    global query_df\n",
    "    query_dict = {'query': [query], 'query_vec' : [np.nan], 'new_query_vec' : [np.nan] }\n",
    "    query_df = pd.DataFrame(data=query_dict, index=[0])\n",
    "    return searchResults.to_json(orient='records')\n",
    "\n",
    "# Feedback\n",
    "@app.route('/feedback', methods=['POST'])\n",
    "def fetchFeedback():\n",
    "    global reranked_df\n",
    "    feedbackJson = request.json['updates']\n",
    "    relevanceList = []\n",
    "    if len(feedbackJson):\n",
    "        for doc in feedbackJson:\n",
    "            for relevance in doc['value']:\n",
    "                relevanceList.append(relevance)\n",
    "    feedback_df = pd.DataFrame(relevanceList)\n",
    "    print(feedback_df.head(10))\n",
    "    reranked_df = get_reranking(feedback_df, 0.3, 0.7)\n",
    "    feedback_df= reranked_df.drop(reranked_df[reranked_df['bntStyle'] == False].index)\n",
    "    return feedback_df.to_json(orient='records')\n",
    "\n",
    "@app.route('/plot')\n",
    "def plot():\n",
    "    # Generate the plot\n",
    "    fig = generate_plot2()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    output = io.BytesIO()\n",
    "    canvas.print_png(output)\n",
    "    response = Response(output.getvalue(), mimetype='image/png')\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0',port=5273)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}