{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "LDA_feature.ipynb",
   "private_outputs": true,
   "provenance": [],
   "authorship_tag": "ABX9TyNbv+U89+kt2/HEZW1AZ3kD",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/govind17/Information-Retrieval-Project/blob/main/LDA_feature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GyY68vB8Rzb7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c8N0IFt3RsEZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.linalg import get_blas_funcs\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "# Import packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from wordcloud import WordCloud \n",
    "from os import path\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ihwTNi84RuAN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def remove_noise(X):\n",
    "        # Convert to list\n",
    "        data = X\n",
    "\n",
    "        # Remove Emails\n",
    "        data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove single quotes\n",
    "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        \n",
    "        def sent_to_words(sentences):\n",
    "            for sentence in sentences:\n",
    "                yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        data_words = list(sent_to_words(data))\n",
    "        \n",
    "        def remove_stopwords(texts):\n",
    "            return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "        \n",
    "        # Remove Stop Words\n",
    "        data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "        def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "            texts_out = []\n",
    "            for sent in texts:\n",
    "                doc = nlp(\" \".join(sent)) \n",
    "                texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "            return texts_out\n",
    "\n",
    "        # Initialize spacy model\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "        # Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "        data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "        \n",
    "        return(data_lemmatized)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-ozDwSlBSRMQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Load data set with class labels and split into train and test set\n",
    "test_size_ratio = 0.2\n",
    "data_Xy = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "category_names = data_Xy.target_names # text names of all categories\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_Xy.data, data_Xy.target, test_size=test_size_ratio, stratify=data_Xy.target)\n",
    "print(\"Training set size: %8d\\tTest set size: %8d\" % (len(train_X), len(test_X)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IjYDAZ3eSXP3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# preprocess train and test text data\n",
    "train_X_clean=remove_noise(train_X)\n",
    "test_X_clean=remove_noise(test_X)\n",
    "x=[]\n",
    "train_X_clean_new=[]\n",
    "for row in train_X_clean:\n",
    "    x=[word for word in row if len(word)>2]\n",
    "    train_X_clean_new.append(x)\n",
    "print(train_X_clean_new[1])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SLfcmhgyVAlL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dictionary_LDA = corpora.Dictionary(train_X_clean_new)\n",
    "\n",
    "# Term Document Frequency\n",
    "train_corpus = [dictionary_LDA.doc2bow(data_lemmatized) for data_lemmatized in train_X_clean_new]\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "            coherence_values = []\n",
    "            model_list = []\n",
    "            for num_topics in range(start, limit, step):\n",
    "                model = gensim.models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "                model_list.append(model)\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "                coherence_values.append(coherencemodel.get_coherence())\n",
    "            return model_list, coherence_values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary_LDA, corpus=train_corpus, texts=train_X_clean_new, start=2, limit=40, step=6)\n",
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8_g7KDOhSfKV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Build LDA model\n",
    "num_topics = 31\n",
    "lda_model = gensim.models.LdaModel(train_corpus, num_topics=num_topics, id2word=dictionary_LDA, passes=10, alpha=[0.01]*num_topics, eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "train_topics = [lda_model[train_corpus[i]] for i in range(len(train_X_clean))]\n",
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "        res = pd.DataFrame(columns=range(num_topics))\n",
    "        for topic_weight in topics_document:\n",
    "            res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "        return res\n",
    "train_features=pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in train_topics]) \\\n",
    "            .reset_index(drop=True).fillna(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ALThFdUpU72j",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_features."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r98Dv13ZX1nN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}